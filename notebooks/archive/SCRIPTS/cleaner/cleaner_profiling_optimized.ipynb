{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.43 s\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "# get_corpus_path\n",
    "# get_txt_orig_path\n",
    "# get_txt_clean_path\n",
    "%run ../path_manager.ipynb\n",
    "\n",
    "\n",
    "# CorpusCleaner\n",
    "%run ../DataCleanerModuleV2.ipynb\n",
    "\n",
    "## Jupyter.notebook.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.22 ms\n"
     ]
    }
   ],
   "source": [
    "USE_JOBLIB_MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.39 ms\n"
     ]
    }
   ],
   "source": [
    "# respeller_cache.clear(warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.85 ms\n"
     ]
    }
   ],
   "source": [
    "def cleaner_test(doc_name='11758940.txt', use_spacy=True):\n",
    "    path = '/home/avsolatorio/WBG/NLP/WB/CORPUS/RAW/eap_files'\n",
    "    \n",
    "    fname = os.path.join(path, doc_name)\n",
    "    \n",
    "    cleaner=Cleaner(\n",
    "        use_spellchecker=True, use_respeller=True, use_lemmatizer=True, use_spacy=use_spacy,\n",
    "        replacements_plurals_to_singular_file='../whitelists/whitelist_replacements_plurals_to_singular.csv',\n",
    "        acronyms_file='../whitelists/whitelist_acronyms.csv',\n",
    "    )\n",
    "    \n",
    "    with open(fname, 'rb') as fl:\n",
    "        text = fl.read()\n",
    "        text = text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    cleaned = cleaner.clean_text(text)\n",
    "    return len(cleaner.respeller.spell_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "# cleaner=Cleaner(\n",
    "#     use_spellchecker=True, use_respeller=True, use_lemmatizer=True, use_spacy=True,\n",
    "#     replacements_plurals_to_singular_file='../whitelists/whitelist_replacements_plurals_to_singular.csv',\n",
    "#     acronyms_file='../whitelists/whitelist_acronyms.csv',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 961 µs\n"
     ]
    }
   ],
   "source": [
    "# %time q = cleaner.spellchecker.set_tokens(['hello', 'world', 'toks'])\n",
    "\n",
    "# errors = set()\n",
    "\n",
    "# for err in cleaner.spellchecker:\n",
    "#     # print (err.word)\n",
    "#     # if err.word not in self.respelled_set:\n",
    "#     errors.add(err.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572, 4915, 3572]\n",
      "time: 23 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Failed lemmatization for None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[0, 524, 149, 3572, 3572, 3572]\n",
      "time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572, 4915, 3572]\n",
      "time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[5387, 3572, 3572, 3572, 5387, 3572]\n",
      "time: 21 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 53, 3095, 3095, 3095]\n",
      "time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572, 4915, 3572]\n",
      "time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "CPU times: user 19.9 s, sys: 3.9 s, total: 23.8 s\n",
      "Wall time: 9.41 s\n",
      "time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%time q = cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572, 4915, 3572]\n",
      "time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572]\n",
      "time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt', '25693850.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572]\n",
      "time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572]\n",
      "time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "CPU times: user 20.3 s, sys: 3.79 s, total: 24.1 s\n",
      "Wall time: 9.61 s\n",
      "time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%time q = cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "CPU times: user 19.5 s, sys: 3.64 s, total: 23.1 s\n",
      "Wall time: 8.69 s\n",
      "time: 8.69 s\n"
     ]
    }
   ],
   "source": [
    "%time q = cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "CPU times: user 1min 19s, sys: 3.92 s, total: 1min 23s\n",
      "Wall time: 1min 8s\n",
      "time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%time q = cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "CPU times: user 19.6 s, sys: 3.59 s, total: 23.2 s\n",
      "Wall time: 8.74 s\n",
      "time: 8.74 s\n"
     ]
    }
   ],
   "source": [
    "%time q = cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[4831, 3471, 3513, 3513, 4831, 3513, 3513, 3513, 4831, 3513, 3513, 3513]\n",
      "time: 38.2 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt'] * 3\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2412, 470, 523, 3513, 2412, 3513, 3513, 3513, 2412, 3513, 3513, 3513]\n",
      "time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt'] * 3\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572]\n",
      "time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2447, 473, 524, 3572]\n",
      "time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did, False) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2412, 470, 523, 3513]\n",
      "time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2412, 470, 523, 3513]\n",
      "time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2412, 470, 523, 3513]\n",
      "time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2412, 470, 523, 3513]\n",
      "time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '9992242.txt', '25693850.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[0, 2412, 470, 523]\n",
      "time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['25693850.txt', '8157124.txt', '9996531.txt', '9992242.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[3038, 2412, 2863, 3080]\n",
      "time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['25693850.txt', '8157124.txt', '9996531.txt', '9992242.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[2412, 470, 3471, 2458]\n",
      "time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['8157124.txt', '9996531.txt', '25693850.txt', '9992242.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "Doclen 18 < 50 = None\n",
      "[3038, 3471, 3471, 3038, 5260]\n",
      "time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "doc_ids = ['25693850.txt', '9996531.txt', '25693850.txt', '25693850.txt', '8157124.txt']\n",
    "\n",
    "with Parallel(n_jobs=2, backend='multiprocessing') as parallel:\n",
    "    proc_spell_cache_lens = parallel(delayed(cleaner_test)(did) for did in doc_ids)\n",
    "print(proc_spell_cache_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.33 ms\n"
     ]
    }
   ],
   "source": [
    "len(Respeller.spell_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "q = cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.58 ms\n"
     ]
    }
   ],
   "source": [
    "q = set(range(10000))\n",
    "%time 'e' in q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.2 ms\n"
     ]
    }
   ],
   "source": [
    "len(Respeller.spell_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3053"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.41 ms\n"
     ]
    }
   ],
   "source": [
    "# Cleaner.spell_cache_dict = Cleaner.spell_cache_manager.dict()\n",
    "len(Cleaner.spell_cache_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 6.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.213871 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: infer_correct_word at line 28\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    28                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    29      3053     105671.0     34.6     49.4          if word not in self.spell_cache:\n",
       "    30                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    31                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    32                                                       self.spell_cache[word] = payload\n",
       "    33                                           \n",
       "    34      3053     108200.0     35.4     50.6          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.228366 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: parallel_infer_correct_word at line 46\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    46                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    47         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    48                                                   \n",
       "    49         1     219240.0 219240.0     96.0          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    50                                           \n",
       "    51         1          2.0      2.0      0.0          words = set([])\n",
       "    52                                           \n",
       "    53      3054       1205.0      0.4      0.5          for res in respell_results:\n",
       "    54      3053       1292.0      0.4      0.6              word = res['word']\n",
       "    55      3053       1290.0      0.4      0.6              correct_word = res['correct_word']\n",
       "    56      3053       1267.0      0.4      0.6              score = res['score']\n",
       "    57                                           \n",
       "    58      3053       1376.0      0.5      0.6              if correct_word and score > self.spell_threshold:\n",
       "    59       777        397.0      0.5      0.2                  if correct_word.istitle():\n",
       "    60                                                               # If the respelling results to a `Title` word\n",
       "    61                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    62       704        364.0      0.5      0.2                      words.add(word)\n",
       "    63                                                           else:\n",
       "    64                                                               # Split and filter since some words are compound terms.\n",
       "    65        73        841.0     11.5      0.4                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    66                                                       else:\n",
       "    67      2276       1090.0      0.5      0.5                  words.add(word)\n",
       "    68                                           \n",
       "    69         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 4.06329 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         32.0     32.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    3949505.0 3949505.0     97.2          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     110770.0 110770.0      2.7          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2726.0   2726.0      0.1          txt_out = ' '.join(txt_out)\n",
       "   162         1        253.0    253.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          0.0      0.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.247625 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3295.0   3295.0      1.3              text_tokens = set(text)\n",
       "   201         1          5.0      5.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13401     231038.0     17.2     93.3              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13400       6697.0      0.5      2.7                  if err.word not in self.respelled_set:\n",
       "   206     13400       6586.0      0.5      2.7                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 1.16438 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     237606.0 237606.0     20.4          text_tokens=word_tokenize(text)\n",
       "   225         1     262472.0 262472.0     22.5          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1     237062.0 237062.0     20.4              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         31.0     31.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79744      25443.0      0.3      2.2          for x in text_tokens:\n",
       "   236     79743      27446.0      0.3      2.4              if (x in errors_set):\n",
       "   237     13065       3914.0      0.3      0.3                  continue\n",
       "   238                                                       \n",
       "   239     66678      24926.0      0.4      2.1              elif x in self.respelled_set:\n",
       "   240       545        217.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       210         87.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       210         81.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66343     289055.0      4.4     24.8              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66341      31027.0      0.5      2.7                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66341      24117.0      0.4      2.1                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          0.0      0.0      0.0          output={}\n",
       "   252         1        893.0    893.0      0.1          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 6.07676 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   327         2          1.0      0.5      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          1.0      0.5      0.0          exp = None\n",
       "   330         2          2.0      1.0      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     294933.0 147466.5      4.9              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       5083.0   2541.5      0.1          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    4115503.0 4115503.0     67.7                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      40109.0  40109.0      0.7                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          4.0      4.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     113814.0 113814.0      1.9                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    1301396.0 1301396.0     21.4                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     205726.0 205726.0      3.4                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          2.0      2.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         74.0     74.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          0.0      0.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          3.0      1.5      0.0          payload = dict(\n",
       "   402         2          2.0      1.0      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   406         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          6.0      3.0      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         53.0     26.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          1.0      0.5      0.0          return payload\n",
       "\n",
       "Total time: 0 s\n",
       "File: <ipython-input-2-de4560781563>\n",
       "Function: wrapper at line 38\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    38                                               def wrapper(*args, **kwargs):\n",
       "    39                                           \n",
       "    40                                                   argument_hash = get_argument_hash(func, args, kwargs)\n",
       "    41                                                   func_id = get_func_fullname(func)\n",
       "    42                                                   \n",
       "    43                                                   fromcache = redis_cache.hget(func_id, argument_hash)\n",
       "    44                                                   \n",
       "    45                                                   if fromcache is None:\n",
       "    46                                                       value = func(*args, **kwargs)\n",
       "    47                                                       tocache = json.dumps(value)\n",
       "    48                                                       \n",
       "    49                                                       # print(func_id, argument_hash)\n",
       "    50                                                       redis_cache.hset(func_id, argument_hash, tocache)\n",
       "    51                                                   else:\n",
       "    52                                                       # Decode since redis returns a byte encoded string\n",
       "    53                                                       fromcache = fromcache.decode('utf-8')\n",
       "    54                                                       value = json.loads(fromcache)\n",
       "    55                                           \n",
       "    56                                                   return value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f cached_infer_correct_word -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 8.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 2.00992 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: infer_correct_word at line 28\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    28                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    29      3053     140226.0     45.9      7.0          if word not in self.spell_cache:\n",
       "    30                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    31      3053    1516822.0    496.8     75.5              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    32      3053     200341.0     65.6     10.0              self.spell_cache[word] = payload\n",
       "    33                                           \n",
       "    34      3053     152533.0     50.0      7.6          return self.spell_cache[word]\n",
       "\n",
       "Total time: 2.03001 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: parallel_infer_correct_word at line 46\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    46                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    47         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    48                                                   \n",
       "    49         1    2020850.0 2020850.0     99.5          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    50                                           \n",
       "    51         1          1.0      1.0      0.0          words = set([])\n",
       "    52                                           \n",
       "    53      3054       1214.0      0.4      0.1          for res in respell_results:\n",
       "    54      3053       1336.0      0.4      0.1              word = res['word']\n",
       "    55      3053       1272.0      0.4      0.1              correct_word = res['correct_word']\n",
       "    56      3053       1309.0      0.4      0.1              score = res['score']\n",
       "    57                                           \n",
       "    58      3053       1357.0      0.4      0.1              if correct_word and score > self.spell_threshold:\n",
       "    59       777        368.0      0.5      0.0                  if correct_word.istitle():\n",
       "    60                                                               # If the respelling results to a `Title` word\n",
       "    61                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    62       704        325.0      0.5      0.0                      words.add(word)\n",
       "    63                                                           else:\n",
       "    64                                                               # Split and filter since some words are compound terms.\n",
       "    65        73        857.0     11.7      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    66                                                       else:\n",
       "    67      2276       1118.0      0.5      0.1                  words.add(word)\n",
       "    68                                           \n",
       "    69         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 4.06338 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         26.0     26.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    3949413.0 3949413.0     97.2          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     110931.0 110931.0      2.7          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2728.0   2728.0      0.1          txt_out = ' '.join(txt_out)\n",
       "   162         1        285.0    285.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.247188 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3300.0   3300.0      1.3              text_tokens = set(text)\n",
       "   201         1          5.0      5.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13401     230796.0     17.2     93.4              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13400       6374.0      0.5      2.6                  if err.word not in self.respelled_set:\n",
       "   206     13400       6708.0      0.5      2.7                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 2.96308 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     237936.0 237936.0      8.0          text_tokens=word_tokenize(text)\n",
       "   225         1     261683.0 261683.0      8.8          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          0.0      0.0      0.0          if errors and self.respeller:\n",
       "   228         1    2038820.0 2038820.0     68.8              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         31.0     31.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79744      25129.0      0.3      0.8          for x in text_tokens:\n",
       "   236     79743      27075.0      0.3      0.9              if (x in errors_set):\n",
       "   237     13065       3869.0      0.3      0.1                  continue\n",
       "   238                                                       \n",
       "   239     66678      24994.0      0.4      0.8              elif x in self.respelled_set:\n",
       "   240       545        231.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       210         97.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       210         74.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66343     287569.0      4.3      9.7              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66341      31087.0      0.5      1.0                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66341      23599.0      0.4      0.8                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          1.0      1.0      0.0          output={}\n",
       "   252         1        879.0    879.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 7.87501 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   327         2          2.0      1.0      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          1.0      0.5      0.0          exp = None\n",
       "   330         2          2.0      1.0      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     297093.0 148546.5      3.8              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       5082.0   2541.0      0.1          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          0.0      0.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    4115235.0 4115235.0     52.3                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      39972.0  39972.0      0.5                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     113273.0 113273.0      1.4                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          0.0      0.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    3102030.0 3102030.0     39.4                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     202137.0 202137.0      2.6                                  token_log = len(word_tokenize(text))\n",
       "   370         1          1.0      1.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         62.0     62.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          0.0      0.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          3.0      1.5      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   406         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          4.0      2.0      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         68.0     34.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          1.0      0.5      0.0          return payload\n",
       "\n",
       "Total time: 1.49997 s\n",
       "File: <ipython-input-2-de4560781563>\n",
       "Function: wrapper at line 38\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    38                                               def wrapper(*args, **kwargs):\n",
       "    39                                           \n",
       "    40      3053     859388.0    281.5     57.3          argument_hash = get_argument_hash(func, args, kwargs)\n",
       "    41      3053     175061.0     57.3     11.7          func_id = get_func_fullname(func)\n",
       "    42                                                   \n",
       "    43      3053     424299.0    139.0     28.3          fromcache = redis_cache.hget(func_id, argument_hash)\n",
       "    44                                                   \n",
       "    45      3053       2444.0      0.8      0.2          if fromcache is None:\n",
       "    46                                                       value = func(*args, **kwargs)\n",
       "    47                                                       tocache = json.dumps(value)\n",
       "    48                                                       \n",
       "    49                                                       # print(func_id, argument_hash)\n",
       "    50                                                       redis_cache.hset(func_id, argument_hash, tocache)\n",
       "    51                                                   else:\n",
       "    52                                                       # Decode since redis returns a byte encoded string\n",
       "    53      3053       3332.0      1.1      0.2              fromcache = fromcache.decode('utf-8')\n",
       "    54      3053      34273.0     11.2      2.3              value = json.loads(fromcache)\n",
       "    55                                           \n",
       "    56      3053       1175.0      0.4      0.1          return value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f cached_infer_correct_word -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 76.497 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: infer_correct_word at line 28\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    28                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    29      3053     187523.0     61.4      0.2          if word not in self.spell_cache:\n",
       "    30                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    31      3053   75744524.0  24809.9     99.0              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    32      3053     342997.0    112.3      0.4              self.spell_cache[word] = payload\n",
       "    33                                           \n",
       "    34      3053     221940.0     72.7      0.3          return self.spell_cache[word]\n",
       "\n",
       "Total time: 76.5214 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: parallel_infer_correct_word at line 46\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    46                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    47         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    48                                                   \n",
       "    49         1   76511889.0 76511889.0    100.0          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    50                                           \n",
       "    51         1          1.0      1.0      0.0          words = set([])\n",
       "    52                                           \n",
       "    53      3054       1242.0      0.4      0.0          for res in respell_results:\n",
       "    54      3053       1521.0      0.5      0.0              word = res['word']\n",
       "    55      3053       1339.0      0.4      0.0              correct_word = res['correct_word']\n",
       "    56      3053       1408.0      0.5      0.0              score = res['score']\n",
       "    57                                           \n",
       "    58      3053       1348.0      0.4      0.0              if correct_word and score > self.spell_threshold:\n",
       "    59       777        378.0      0.5      0.0                  if correct_word.istitle():\n",
       "    60                                                               # If the respelling results to a `Title` word\n",
       "    61                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    62       704        350.0      0.5      0.0                      words.add(word)\n",
       "    63                                                           else:\n",
       "    64                                                               # Split and filter since some words are compound terms.\n",
       "    65        73        833.0     11.4      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    66                                                       else:\n",
       "    67      2276       1112.0      0.5      0.0                  words.add(word)\n",
       "    68                                           \n",
       "    69         1          0.0      0.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 4.52076 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         98.0     98.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    4408796.0 4408796.0     97.5          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     108855.0 108855.0      2.4          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2764.0   2764.0      0.1          txt_out = ' '.join(txt_out)\n",
       "   162         1        247.0    247.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.322135 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3331.0   3331.0      1.0              text_tokens = set(text)\n",
       "   201         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13401     304419.0     22.7     94.5              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13400       7237.0      0.5      2.2                  if err.word not in self.respelled_set:\n",
       "   206     13400       7138.0      0.5      2.2                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 77.5363 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     246549.0 246549.0      0.3          text_tokens=word_tokenize(text)\n",
       "   225         1     337790.0 337790.0      0.4          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1   76530255.0 76530255.0     98.7              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         28.0     28.0      0.0          errors_set=set(errors)\n",
       "   233         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79744      24481.0      0.3      0.0          for x in text_tokens:\n",
       "   236     79743      27158.0      0.3      0.0              if (x in errors_set):\n",
       "   237     13065       3843.0      0.3      0.0                  continue\n",
       "   238                                                       \n",
       "   239     66678      24214.0      0.4      0.0              elif x in self.respelled_set:\n",
       "   240       545        222.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       210         95.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       210         76.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66343     286124.0      4.3      0.4              elif (x in self.stopwords):\n",
       "   245         2          2.0      1.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66341      30775.0      0.5      0.0                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66341      23845.0      0.4      0.0                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          0.0      0.0      0.0          output={}\n",
       "   252         1        806.0    806.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 83.3651 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   327         2          1.0      0.5      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          2.0      1.0      0.0          exp = None\n",
       "   330         2          1.0      0.5      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     311296.0 155648.0      0.4              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       6727.0   3363.5      0.0          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    4574753.0 4574753.0      5.5                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      41086.0  41086.0      0.0                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          2.0      2.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     554889.0 554889.0      0.7                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1   77672917.0 77672917.0     93.2                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     203264.0 203264.0      0.2                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          2.0      2.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         75.0     75.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          3.0      1.5      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          2.0      1.0      0.0              token=token_log,\n",
       "   404         2          1.0      0.5      0.0              text=text_log,\n",
       "   405         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   406         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   407         2          1.0      0.5      0.0              exception=exp,\n",
       "   408         2          4.0      2.0      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         64.0     32.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          1.0      0.5      0.0          return payload\n",
       "\n",
       "Total time: 58.8061 s\n",
       "File: <ipython-input-2-de4560781563>\n",
       "Function: wrapper at line 38\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    38                                               def wrapper(*args, **kwargs):\n",
       "    39                                           \n",
       "    40      5745    1476879.0    257.1      2.5          argument_hash = get_argument_hash(func, args, kwargs)\n",
       "    41      5745     326380.0     56.8      0.6          func_id = get_func_fullname(func)\n",
       "    42                                                   \n",
       "    43      5745    1075068.0    187.1      1.8          fromcache = redis_cache.hget(func_id, argument_hash)\n",
       "    44                                                   \n",
       "    45      5745       4443.0      0.8      0.0          if fromcache is None:\n",
       "    46      5745   54571906.0   9499.0     92.8              value = func(*args, **kwargs)\n",
       "    47      5745     104846.0     18.2      0.2              tocache = json.dumps(value)\n",
       "    48                                                       \n",
       "    49                                                       # print(func_id, argument_hash)\n",
       "    50      5745    1241707.0    216.1      2.1              redis_cache.hset(func_id, argument_hash, tocache)\n",
       "    51                                                   else:\n",
       "    52                                                       # Decode since redis returns a byte encoded string\n",
       "    53                                                       fromcache = fromcache.decode('utf-8')\n",
       "    54                                                       value = json.loads(fromcache)\n",
       "    55                                           \n",
       "    56      5745       4904.0      0.9      0.0          return value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f cached_infer_correct_word -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 76.5973 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: infer_correct_word at line 28\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    28                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    29      3053     222779.0     73.0      0.3          if word not in self.spell_cache:\n",
       "    30                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    31      3053   75759353.0  24814.7     98.9              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    32      3053     361525.0    118.4      0.5              self.spell_cache[word] = payload\n",
       "    33                                           \n",
       "    34      3053     253600.0     83.1      0.3          return self.spell_cache[word]\n",
       "\n",
       "Total time: 76.6219 s\n",
       "File: <ipython-input-2-09e260770320>\n",
       "Function: parallel_infer_correct_word at line 46\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    46                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    47         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    48                                                   \n",
       "    49         1   76611841.0 76611841.0    100.0          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    50                                           \n",
       "    51         1          1.0      1.0      0.0          words = set([])\n",
       "    52                                           \n",
       "    53      3054       1319.0      0.4      0.0          for res in respell_results:\n",
       "    54      3053       1623.0      0.5      0.0              word = res['word']\n",
       "    55      3053       1416.0      0.5      0.0              correct_word = res['correct_word']\n",
       "    56      3053       1473.0      0.5      0.0              score = res['score']\n",
       "    57                                           \n",
       "    58      3053       1449.0      0.5      0.0              if correct_word and score > self.spell_threshold:\n",
       "    59       777        398.0      0.5      0.0                  if correct_word.istitle():\n",
       "    60                                                               # If the respelling results to a `Title` word\n",
       "    61                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    62       704        373.0      0.5      0.0                      words.add(word)\n",
       "    63                                                           else:\n",
       "    64                                                               # Split and filter since some words are compound terms.\n",
       "    65        73        845.0     11.6      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    66                                                       else:\n",
       "    67      2276       1174.0      0.5      0.0                  words.add(word)\n",
       "    68                                           \n",
       "    69         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 4.37246 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         98.0     98.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    4260048.0 4260048.0     97.4          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     109314.0 109314.0      2.5          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2744.0   2744.0      0.1          txt_out = ' '.join(txt_out)\n",
       "   162         1        254.0    254.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.297779 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3181.0   3181.0      1.1              text_tokens = set(text)\n",
       "   201         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13401     280005.0     20.9     94.0              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13400       7282.0      0.5      2.4                  if err.word not in self.respelled_set:\n",
       "   206     13400       7300.0      0.5      2.5                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 77.6072 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     245504.0 245504.0      0.3          text_tokens=word_tokenize(text)\n",
       "   225         1     312980.0 312980.0      0.4          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1   76631673.0 76631673.0     98.7              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          3.0      3.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         28.0     28.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79744      24680.0      0.3      0.0          for x in text_tokens:\n",
       "   236     79743      26559.0      0.3      0.0              if (x in errors_set):\n",
       "   237     13065       3776.0      0.3      0.0                  continue\n",
       "   238                                                       \n",
       "   239     66678      24152.0      0.4      0.0              elif x in self.respelled_set:\n",
       "   240       545        240.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       210         91.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       210         79.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66343     282135.0      4.3      0.4              elif (x in self.stopwords):\n",
       "   245         2          1.0      0.5      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66341      30481.0      0.5      0.0                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66341      24006.0      0.4      0.0                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          0.0      0.0      0.0          output={}\n",
       "   252         1        830.0    830.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 83.2553 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   327         2          2.0      1.0      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          2.0      1.0      0.0          exp = None\n",
       "   330         2          0.0      0.0      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     309269.0 154634.5      0.4              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       6572.0   3286.0      0.0          text = text.lower()\n",
       "   336         2          5.0      2.5      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    4427282.0 4427282.0      5.3                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      41385.0  41385.0      0.0                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     518880.0 518880.0      0.6                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1   77743530.0 77743530.0     93.4                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     208115.0 208115.0      0.2                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         74.0     74.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          2.0      1.0      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          0.0      0.0      0.0              skipped=skipped_log,\n",
       "   406         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2        101.0     50.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          1.0      0.5      0.0          return payload\n",
       "\n",
       "Total time: 58.6884 s\n",
       "File: <ipython-input-2-de4560781563>\n",
       "Function: wrapper at line 38\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    38                                               def wrapper(*args, **kwargs):\n",
       "    39                                           \n",
       "    40      5745    1494378.0    260.1      2.5          argument_hash = get_argument_hash(func, args, kwargs)\n",
       "    41      5745     328160.0     57.1      0.6          func_id = get_func_fullname(func)\n",
       "    42                                                   \n",
       "    43      5745     948904.0    165.2      1.6          fromcache = redis_cache.hget(func_id, argument_hash)\n",
       "    44                                                   \n",
       "    45      5745       4439.0      0.8      0.0          if fromcache is None:\n",
       "    46      5745   54656474.0   9513.7     93.1              value = func(*args, **kwargs)\n",
       "    47      5745     104556.0     18.2      0.2              tocache = json.dumps(value)\n",
       "    48                                                       \n",
       "    49                                                       # print(func_id, argument_hash)\n",
       "    50      5745    1146919.0    199.6      2.0              redis_cache.hset(func_id, argument_hash, tocache)\n",
       "    51                                                   else:\n",
       "    52                                                       # Decode since redis returns a byte encoded string\n",
       "    53                                                       fromcache = fromcache.decode('utf-8')\n",
       "    54                                                       value = json.loads(fromcache)\n",
       "    55                                           \n",
       "    56      5745       4597.0      0.8      0.0          return value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f cached_infer_correct_word -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 14.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 7.25653 s\n",
       "File: <ipython-input-2-0caae9fe0792>\n",
       "Function: infer_correct_word at line 28\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    28                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    29      3053       2127.0      0.7      0.0          if word not in self.spell_cache:\n",
       "    30                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    31      3053    7248394.0   2374.2     99.9              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    32      3053       4293.0      1.4      0.1              self.spell_cache[word] = payload\n",
       "    33                                           \n",
       "    34      3053       1720.0      0.6      0.0          return self.spell_cache[word]\n",
       "\n",
       "Total time: 7.27643 s\n",
       "File: <ipython-input-2-0caae9fe0792>\n",
       "Function: parallel_infer_correct_word at line 46\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    46                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    47         1          2.0      2.0      0.0          respelled_set = {}\n",
       "    48                                                   \n",
       "    49         1    7267148.0 7267148.0     99.9          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    50                                           \n",
       "    51         1          1.0      1.0      0.0          words = set([])\n",
       "    52                                           \n",
       "    53      3054       1209.0      0.4      0.0          for res in respell_results:\n",
       "    54      3053       1387.0      0.5      0.0              word = res['word']\n",
       "    55      3053       1294.0      0.4      0.0              correct_word = res['correct_word']\n",
       "    56      3053       1318.0      0.4      0.0              score = res['score']\n",
       "    57                                           \n",
       "    58      3053       1366.0      0.4      0.0              if correct_word and score > self.spell_threshold:\n",
       "    59       777        403.0      0.5      0.0                  if correct_word.istitle():\n",
       "    60                                                               # If the respelling results to a `Title` word\n",
       "    61                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    62       704        386.0      0.5      0.0                      words.add(word)\n",
       "    63                                                           else:\n",
       "    64                                                               # Split and filter since some words are compound terms.\n",
       "    65        73        827.0     11.3      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    66                                                       else:\n",
       "    67      2276       1090.0      0.5      0.0                  words.add(word)\n",
       "    68                                           \n",
       "    69         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 4.31928 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         95.0     95.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    4206765.0 4206765.0     97.4          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     109400.0 109400.0      2.5          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2777.0   2777.0      0.1          txt_out = ' '.join(txt_out)\n",
       "   162         1        247.0    247.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.290584 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3178.0   3178.0      1.1              text_tokens = set(text)\n",
       "   201         1          5.0      5.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13401     273903.0     20.4     94.3              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13400       6719.0      0.5      2.3                  if err.word not in self.respelled_set:\n",
       "   206     13400       6774.0      0.5      2.3                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 8.25327 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     248299.0 248299.0      3.0          text_tokens=word_tokenize(text)\n",
       "   225         1     304076.0 304076.0      3.7          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1    7284743.0 7284743.0     88.3              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         31.0     31.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79744      24752.0      0.3      0.3          for x in text_tokens:\n",
       "   236     79743      26526.0      0.3      0.3              if (x in errors_set):\n",
       "   237     13065       3821.0      0.3      0.0                  continue\n",
       "   238                                                       \n",
       "   239     66678      24448.0      0.4      0.3              elif x in self.respelled_set:\n",
       "   240       545        195.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       210        100.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       210         82.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66343     281107.0      4.2      3.4              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66341      30724.0      0.5      0.4                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66341      23490.0      0.4      0.3                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          1.0      1.0      0.0          output={}\n",
       "   252         1        862.0    862.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          1.0      1.0      0.0          return output\n",
       "\n",
       "Total time: 13.8451 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   327         2          1.0      0.5      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          2.0      1.0      0.0          exp = None\n",
       "   330         2          2.0      1.0      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     309278.0 154639.0      2.2              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       6778.0   3389.0      0.0          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    4373640.0 4373640.0     31.6                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      41452.0  41452.0      0.3                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     521987.0 521987.0      3.8                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          3.0      3.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    8388492.0 8388492.0     60.6                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     203261.0 203261.0      1.5                                  token_log = len(word_tokenize(text))\n",
       "   370         1          3.0      3.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         72.0     72.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          0.0      0.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          3.0      1.5      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   406         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          6.0      3.0      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         71.0     35.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f cached_infer_correct_word -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 9.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 7.3732 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         29.0     29.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7229098.0 7229098.0     98.0          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     140957.0 140957.0      1.9          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2862.0   2862.0      0.0          txt_out = ' '.join(txt_out)\n",
       "   162         1        249.0    249.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.244253 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3232.0   3232.0      1.3              text_tokens = set(text)\n",
       "   201         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13333     227822.0     17.1     93.3              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13332       6734.0      0.5      2.8                  if err.word not in self.respelled_set:\n",
       "   206     13332       6455.0      0.5      2.6                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 0.93797 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     238178.0 238178.0     25.4          text_tokens=word_tokenize(text)\n",
       "   225         1     258319.0 258319.0     27.5          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1      23201.0  23201.0      2.5              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         31.0     31.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79643      24576.0      0.3      2.6          for x in text_tokens:\n",
       "   236     79642      26486.0      0.3      2.8              if (x in errors_set):\n",
       "   237     13001       3828.0      0.3      0.4                  continue\n",
       "   238                                                       \n",
       "   239     66641      24425.0      0.4      2.6              elif x in self.respelled_set:\n",
       "   240       537        214.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       206         82.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       206         82.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66310     283736.0      4.3     30.3              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66308      30617.0      0.5      3.3                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66308      23277.0      0.4      2.5                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          1.0      1.0      0.0          output={}\n",
       "   252         1        908.0    908.0      0.1          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          1.0      1.0      0.0          return output\n",
       "\n",
       "Total time: 9.1617 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          1.0      0.5      0.0          token_log = 0\n",
       "   326         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   327         2          2.0      1.0      0.0          text_log = ''\n",
       "   328         2          1.0      0.5      0.0          spell_errors = []\n",
       "   329         2          1.0      0.5      0.0          exp = None\n",
       "   330         2          2.0      1.0      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          1.0      0.5      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     295940.0 147970.0      3.2              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       5687.0   2843.5      0.1          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          4.0      2.0      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          0.0      0.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    7427237.0 7427237.0     81.1                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      40907.0  40907.0      0.4                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          0.0      0.0      0.0                      try:\n",
       "   356         1     114013.0 114013.0      1.2                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    1074931.0 1074931.0     11.7                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     202796.0 202796.0      2.2                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          2.0      2.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         61.0     61.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          2.0      1.0      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          2.0      1.0      0.0              token=token_log,\n",
       "   404         2          1.0      0.5      0.0              text=text_log,\n",
       "   405         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   406         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   407         2          1.0      0.5      0.0              exception=exp,\n",
       "   408         2          6.0      3.0      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         66.0     33.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          2.0      1.0      0.0          return payload\n",
       "\n",
       "Total time: 0.002313 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1270.0      0.4     54.9          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31                                                       self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1043.0      0.3     45.1          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.014812 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1       5688.0   5688.0     38.4          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1179.0      0.4      8.0          for res in respell_results:\n",
       "    53      3038       1362.0      0.4      9.2              word = res['word']\n",
       "    54      3038       1322.0      0.4      8.9              correct_word = res['correct_word']\n",
       "    55      3038       1327.0      0.4      9.0              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1327.0      0.4      9.0              if correct_word and score > self.spell_threshold:\n",
       "    58       772        390.0      0.5      2.6                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        343.0      0.5      2.3                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        836.0     11.5      5.6                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1035.0      0.5      7.0                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 17.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 7.56751 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         78.0     78.0      0.0          stopwords = set(self.stopwords)\n",
       "   152                                           #         try:\n",
       "   153                                           #             lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                           #         except OSError:\n",
       "   155                                           #             lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7421046.0 7421046.0     98.1          doc = _lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     143316.0 143316.0      1.9          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2810.0   2810.0      0.0          txt_out = ' '.join(txt_out)\n",
       "   162         1        258.0    258.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.294498 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3246.0   3246.0      1.1              text_tokens = set(text)\n",
       "   201         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13333     278148.0     20.9     94.4              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13332       6828.0      0.5      2.3                  if err.word not in self.respelled_set:\n",
       "   206     13332       6265.0      0.5      2.1                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 8.11711 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     244451.0 244451.0      3.0          text_tokens=word_tokenize(text)\n",
       "   225         1     308011.0 308011.0      3.8          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          2.0      2.0      0.0          if errors and self.respeller:\n",
       "   228         1    7149113.0 7149113.0     88.1              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         32.0     32.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79643      24182.0      0.3      0.3          for x in text_tokens:\n",
       "   236     79642      26173.0      0.3      0.3              if (x in errors_set):\n",
       "   237     13001       3710.0      0.3      0.0                  continue\n",
       "   238                                                       \n",
       "   239     66641      24576.0      0.4      0.3              elif x in self.respelled_set:\n",
       "   240       537        200.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       206         73.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       206         79.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66310     282734.0      4.3      3.5              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66308      30014.0      0.5      0.4                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66308      22893.0      0.3      0.3                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          1.0      1.0      0.0          output={}\n",
       "   252         1        856.0    856.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 16.9061 s\n",
       "File: <ipython-input-2-54d4f5278dc4>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   327         2          1.0      0.5      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          2.0      1.0      0.0          exp = None\n",
       "   330         2          1.0      0.5      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          1.0      0.5      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     308170.0 154085.0      1.8              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       6638.0   3319.0      0.0          text = text.lower()\n",
       "   336         2          5.0      2.5      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          5.0      2.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    7623340.0 7623340.0     45.1                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          1.0      1.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      40735.0  40735.0      0.2                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          2.0      2.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     472000.0 472000.0      2.8                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    8252410.0 8252410.0     48.8                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     202655.0 202655.0      1.2                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         54.0     54.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          2.0      1.0      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   406         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          7.0      3.5      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         65.0     32.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          1.0      0.5      0.0          return payload\n",
       "\n",
       "Total time: 7.12067 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1859.0      0.6      0.0          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30      3038    7112994.0   2341.3     99.9              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31      3038       3992.0      1.3      0.1              self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1828.0      0.6      0.0          return self.spell_cache[word]\n",
       "\n",
       "Total time: 7.14075 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1    7131622.0 7131622.0     99.9          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1197.0      0.4      0.0          for res in respell_results:\n",
       "    53      3038       1358.0      0.4      0.0              word = res['word']\n",
       "    54      3038       1308.0      0.4      0.0              correct_word = res['correct_word']\n",
       "    55      3038       1302.0      0.4      0.0              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1325.0      0.4      0.0              if correct_word and score > self.spell_threshold:\n",
       "    58       772        372.0      0.5      0.0                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        353.0      0.5      0.0                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        848.0     11.6      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1061.0      0.5      0.0                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          0.0      0.0      0.0          return words, respelled_set"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 9.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.002373 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1281.0      0.4     54.0          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31                                                       self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1092.0      0.4     46.0          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.014641 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1       5660.0   5660.0     38.7          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          2.0      2.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1215.0      0.4      8.3          for res in respell_results:\n",
       "    53      3038       1336.0      0.4      9.1              word = res['word']\n",
       "    54      3038       1290.0      0.4      8.8              correct_word = res['correct_word']\n",
       "    55      3038       1268.0      0.4      8.7              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1325.0      0.4      9.0              if correct_word and score > self.spell_threshold:\n",
       "    58       772        391.0      0.5      2.7                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        317.0      0.5      2.2                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        797.0     10.9      5.4                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1039.0      0.5      7.1                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          0.0      0.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 7.72501 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         27.0     27.0      0.0          stopwords = set(self.stopwords)\n",
       "   152         1          1.0      1.0      0.0          try:\n",
       "   153         1     220689.0 220689.0      2.9              lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                                   except OSError:\n",
       "   155                                                       lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7359959.0 7359959.0     95.3          doc = lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     141269.0 141269.0      1.8          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2818.0   2818.0      0.0          txt_out = ' '.join(txt_out)\n",
       "   162         1        246.0    246.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.242254 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          2.0      2.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3223.0   3223.0      1.3              text_tokens = set(text)\n",
       "   201         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13333     225041.0     16.9     92.9              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13332       6862.0      0.5      2.8                  if err.word not in self.respelled_set:\n",
       "   206     13332       7117.0      0.5      2.9                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          0.0      0.0      0.0          return errors\n",
       "\n",
       "Total time: 0.934258 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     238240.0 238240.0     25.5          text_tokens=word_tokenize(text)\n",
       "   225         1     255942.0 255942.0     27.4          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1      23303.0  23303.0      2.5              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         29.0     29.0      0.0          errors_set=set(errors)\n",
       "   233         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79643      24726.0      0.3      2.6          for x in text_tokens:\n",
       "   236     79642      26694.0      0.3      2.9              if (x in errors_set):\n",
       "   237     13001       3713.0      0.3      0.4                  continue\n",
       "   238                                                       \n",
       "   239     66641      23826.0      0.4      2.6              elif x in self.respelled_set:\n",
       "   240       537        196.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       206        103.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       206         72.0      0.3      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66310     281966.0      4.3     30.2              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66308      30809.0      0.5      3.3                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66308      23683.0      0.4      2.5                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          1.0      1.0      0.0          output={}\n",
       "   252         1        947.0    947.0      0.1          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          1.0      1.0      0.0          return output\n",
       "\n",
       "Total time: 9.52223 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          0.0      0.0      0.0          skipped_log = ''\n",
       "   327         2          2.0      1.0      0.0          text_log = ''\n",
       "   328         2          1.0      0.5      0.0          spell_errors = []\n",
       "   329         2          2.0      1.0      0.0          exp = None\n",
       "   330         2          1.0      0.5      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     304236.0 152118.0      3.2              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       6453.0   3226.5      0.1          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          2.0      1.0      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    7788210.0 7788210.0     81.8                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          2.0      2.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      40053.0  40053.0      0.4                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     108496.0 108496.0      1.1                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    1070920.0 1070920.0     11.2                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          2.0      2.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     203664.0 203664.0      2.1                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          2.0      2.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         62.0     62.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          1.0      0.5      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          3.0      1.5      0.0          payload = dict(\n",
       "   402         2          2.0      1.0      0.0              lang=lang_log,\n",
       "   403         2          0.0      0.0      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   406         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          6.0      3.0      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         69.0     34.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 21.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1.53162 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1660.0      0.5      0.1          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30      3038    1525164.0    502.0     99.6              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31      3038       3363.0      1.1      0.2              self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1431.0      0.5      0.1          return self.spell_cache[word]\n",
       "\n",
       "Total time: 1.54965 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1    1540414.0 1540414.0     99.4          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          2.0      2.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1232.0      0.4      0.1          for res in respell_results:\n",
       "    53      3038       1326.0      0.4      0.1              word = res['word']\n",
       "    54      3038       1308.0      0.4      0.1              correct_word = res['correct_word']\n",
       "    55      3038       1282.0      0.4      0.1              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1377.0      0.5      0.1              if correct_word and score > self.spell_threshold:\n",
       "    58       772        379.0      0.5      0.0                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        361.0      0.5      0.0                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        850.0     11.6      0.1                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1120.0      0.5      0.1                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          0.0      0.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 17.3117 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         86.0     86.0      0.0          stopwords = set(self.stopwords)\n",
       "   152         1          0.0      0.0      0.0          try:\n",
       "   153         1    9823912.0 9823912.0     56.7              lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                                   except OSError:\n",
       "   155                                                       lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7342106.0 7342106.0     42.4          doc = lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     142516.0 142516.0      0.8          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2784.0   2784.0      0.0          txt_out = ' '.join(txt_out)\n",
       "   162         1        254.0    254.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.28827 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3184.0   3184.0      1.1              text_tokens = set(text)\n",
       "   201         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13333     271558.0     20.4     94.2              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13332       6702.0      0.5      2.3                  if err.word not in self.respelled_set:\n",
       "   206     13332       6815.0      0.5      2.4                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 2.52109 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     244882.0 244882.0      9.7          text_tokens=word_tokenize(text)\n",
       "   225         1     301881.0 301881.0     12.0          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          2.0      2.0      0.0          if errors and self.respeller:\n",
       "   228         1    1558482.0 1558482.0     61.8              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          6.0      6.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         34.0     34.0      0.0          errors_set=set(errors)\n",
       "   233         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79643      24473.0      0.3      1.0          for x in text_tokens:\n",
       "   236     79642      26405.0      0.3      1.0              if (x in errors_set):\n",
       "   237     13001       3698.0      0.3      0.1                  continue\n",
       "   238                                                       \n",
       "   239     66641      24185.0      0.4      1.0              elif x in self.respelled_set:\n",
       "   240       537        218.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       206         99.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       206         66.0      0.3      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66310     281772.0      4.2     11.2              elif (x in self.stopwords):\n",
       "   245         2          2.0      1.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66308      30518.0      0.5      1.2                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66308      23551.0      0.4      0.9                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          1.0      1.0      0.0          output={}\n",
       "   252         1        813.0    813.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 21.1055 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   327         2          1.0      0.5      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          1.0      0.5      0.0          exp = None\n",
       "   330         2          1.0      0.5      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     306948.0 153474.0      1.5              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       7206.0   3603.0      0.0          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1   17377103.0 17377103.0     82.3                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          2.0      2.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      41350.0  41350.0      0.2                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     512743.0 512743.0      2.4                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   361         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    2657284.0 2657284.0     12.6                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     202646.0 202646.0      1.0                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          2.0      2.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         56.0     56.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          2.0      1.0      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          2.0      1.0      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   406         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         65.0     32.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 10.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.002353 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1314.0      0.4     55.8          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31                                                       self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1039.0      0.3     44.2          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.014607 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1       5729.0   5729.0     39.2          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          2.0      2.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1181.0      0.4      8.1          for res in respell_results:\n",
       "    53      3038       1295.0      0.4      8.9              word = res['word']\n",
       "    54      3038       1250.0      0.4      8.6              correct_word = res['correct_word']\n",
       "    55      3038       1236.0      0.4      8.5              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1355.0      0.4      9.3              if correct_word and score > self.spell_threshold:\n",
       "    58       772        378.0      0.5      2.6                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        354.0      0.5      2.4                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        829.0     11.4      5.7                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266        996.0      0.4      6.8                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 7.74292 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         28.0     28.0      0.0          stopwords = set(self.stopwords)\n",
       "   152         1          0.0      0.0      0.0          try:\n",
       "   153         1     219295.0 219295.0      2.8              lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                                   except OSError:\n",
       "   155                                                       lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7374114.0 7374114.0     95.2          doc = lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     146409.0 146409.0      1.9          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2776.0   2776.0      0.0          txt_out = ' '.join(txt_out)\n",
       "   162         1        293.0    293.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          1.0      1.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.979125 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3243.0   3243.0      0.3              text_tokens = set(text)\n",
       "   201         1          4.0      4.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13333     961595.0     72.1     98.2              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13332       7335.0      0.6      0.7                  if err.word not in self.respelled_set:\n",
       "   206     13332       6944.0      0.5      0.7                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 1.67776 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     238030.0 238030.0     14.2          text_tokens=word_tokenize(text)\n",
       "   225         1     992905.0 992905.0     59.2          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          0.0      0.0      0.0          if errors and self.respeller:\n",
       "   228         1      22848.0  22848.0      1.4              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         29.0     29.0      0.0          errors_set=set(errors)\n",
       "   233         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79643      25297.0      0.3      1.5          for x in text_tokens:\n",
       "   236     79642      27632.0      0.3      1.6              if (x in errors_set):\n",
       "   237     13001       3949.0      0.3      0.2                  continue\n",
       "   238                                                       \n",
       "   239     66641      25059.0      0.4      1.5              elif x in self.respelled_set:\n",
       "   240       537        220.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       206         98.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       206         82.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66310     283707.0      4.3     16.9              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66308      32291.0      0.5      1.9                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66308      24715.0      0.4      1.5                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          0.0      0.0      0.0          output={}\n",
       "   252         1        893.0    893.0      0.1          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 10.2883 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   327         2          2.0      1.0      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          1.0      0.5      0.0          exp = None\n",
       "   330         2          1.0      0.5      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     294121.0 147060.5      2.9              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       5549.0   2774.5      0.1          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          0.0      0.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1    7805625.0 7805625.0     75.9                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          2.0      2.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      41171.0  41171.0      0.4                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          1.0      1.0      0.0                      try:\n",
       "   356         1     110710.0 110710.0      1.1                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   361         1          4.0      4.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          0.0      0.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    1827939.0 1827939.0     17.8                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          2.0      2.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     203017.0 203017.0      2.0                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         62.0     62.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          1.0      0.5      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          3.0      1.5      0.0          payload = dict(\n",
       "   402         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   403         2          1.0      0.5      0.0              token=token_log,\n",
       "   404         2          1.0      0.5      0.0              text=text_log,\n",
       "   405         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   406         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   407         2          1.0      0.5      0.0              exception=exp,\n",
       "   408         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         66.0     33.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 22.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1.56811 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1726.0      0.6      0.1          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30      3038    1561576.0    514.0     99.6              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31      3038       3346.0      1.1      0.2              self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1466.0      0.5      0.1          return self.spell_cache[word]\n",
       "\n",
       "Total time: 1.58602 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1    1576845.0 1576845.0     99.4          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          2.0      2.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1216.0      0.4      0.1          for res in respell_results:\n",
       "    53      3038       1307.0      0.4      0.1              word = res['word']\n",
       "    54      3038       1283.0      0.4      0.1              correct_word = res['correct_word']\n",
       "    55      3038       1291.0      0.4      0.1              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1368.0      0.5      0.1              if correct_word and score > self.spell_threshold:\n",
       "    58       772        390.0      0.5      0.0                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        359.0      0.5      0.0                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        863.0     11.8      0.1                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1094.0      0.5      0.1                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 17.5478 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         95.0     95.0      0.0          stopwords = set(self.stopwords)\n",
       "   152         1          0.0      0.0      0.0          try:\n",
       "   153         1    9983949.0 9983949.0     56.9              lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                                   except OSError:\n",
       "   155                                                       lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7412078.0 7412078.0     42.2          doc = lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1     148580.0 148580.0      0.8          txt_out = [token.lemma_ for token in doc if token.lemma_ not in stopwords]\n",
       "   161         1       2803.0   2803.0      0.0          txt_out = ' '.join(txt_out)\n",
       "   162         1        250.0    250.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   163                                           \n",
       "   164         1          0.0      0.0      0.0          return txt_out\n",
       "\n",
       "Total time: 0.979864 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: get_misspelled_tokens at line 183\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   183                                               def get_misspelled_tokens(self, text):\n",
       "   184         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   185                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   186                                                   \n",
       "   187         1          2.0      2.0      0.0          errors = set([])\n",
       "   188                                           \n",
       "   189                                           #         if ENCHANT_INSTALLED:\n",
       "   190                                           #             # Input is a text\n",
       "   191                                           #             self.spellchecker.set_text(text)\n",
       "   192                                           \n",
       "   193                                           #             for err in self.spellchecker:\n",
       "   194                                           #                 #print (err.word)\n",
       "   195                                           #                 if err.word not in self.respelled_set:\n",
       "   196                                           #                     errors.add(err.word)\n",
       "   197                                           \n",
       "   198         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   199                                                       # Input is a list of tokens\n",
       "   200         1       3219.0   3219.0      0.3              text_tokens = set(text)\n",
       "   201         1          5.0      5.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   202                                           \n",
       "   203     13333     962457.0     72.2     98.2              for err in self.spellchecker:\n",
       "   204                                                           # print (err.word)\n",
       "   205     13332       7342.0      0.6      0.7                  if err.word not in self.respelled_set:\n",
       "   206     13332       6837.0      0.5      0.7                      errors.add(err.word)\n",
       "   207                                                   else:\n",
       "   208                                                       # Input is a list of tokens\n",
       "   209                                                       text_tokens = set(text)\n",
       "   210                                                       for token in text_tokens:\n",
       "   211                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   212                                                           \n",
       "   213                                                           # If suggestions are available, make sure that the first\n",
       "   214                                                           # suggestion is similar to the token to make sure\n",
       "   215                                                           # that the token being testing is a legit word.\n",
       "   216                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   217                                                               continue\n",
       "   218                                                           else:\n",
       "   219                                                               errors.add(token)\n",
       "   220         1          0.0      0.0      0.0          return errors\n",
       "\n",
       "Total time: 3.25527 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: spellcheck_text at line 223\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   223                                               def spellcheck_text(self, text):\n",
       "   224         1     244901.0 244901.0      7.5          text_tokens=word_tokenize(text)\n",
       "   225         1     993855.0 993855.0     30.5          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   226                                           \n",
       "   227         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   228         1    1594414.0 1594414.0     49.0              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   229                                                       # print(respelled_set)\n",
       "   230         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   231                                           \n",
       "   232         1         31.0     31.0      0.0          errors_set=set(errors)\n",
       "   233         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   234                                                   \n",
       "   235     79643      25202.0      0.3      0.8          for x in text_tokens:\n",
       "   236     79642      27226.0      0.3      0.8              if (x in errors_set):\n",
       "   237     13001       4009.0      0.3      0.1                  continue\n",
       "   238                                                       \n",
       "   239     66641      24974.0      0.4      0.8              elif x in self.respelled_set:\n",
       "   240       537        229.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   241       206         76.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   242       206         82.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   243                                           \n",
       "   244     66310     282908.0      4.3      8.7              elif (x in self.stopwords):\n",
       "   245         2          0.0      0.0      0.0                  continue\n",
       "   246                                           \n",
       "   247                                                       else:\n",
       "   248     66308      31846.0      0.5      1.0                  x = self.plural_singular_map.get(x, x)\n",
       "   249     66308      24704.0      0.4      0.8                  cleaned_text.append(x)\n",
       "   250                                           \n",
       "   251         1          0.0      0.0      0.0          output={}\n",
       "   252         1        811.0    811.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   253         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   254                                              \n",
       "   255         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 22.1036 s\n",
       "File: <ipython-input-2-f34f86336c05>\n",
       "Function: clean_text at line 323\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   323                                               def clean_text(self, text, filen=None):\n",
       "   324         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   325         2          2.0      1.0      0.0          token_log = 0\n",
       "   326         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   327         2          2.0      1.0      0.0          text_log = ''\n",
       "   328         2          2.0      1.0      0.0          spell_errors = []\n",
       "   329         2          2.0      1.0      0.0          exp = None\n",
       "   330         2          1.0      0.5      0.0          write_status = False\n",
       "   331                                                   \n",
       "   332         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   333         2     307184.0 153592.0      1.4              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   334                                           \n",
       "   335         2       7253.0   3626.5      0.0          text = text.lower()\n",
       "   336         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   337                                                   \n",
       "   338         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   339                                                       \n",
       "   340         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   341                                                           # Apply lemmatizer\n",
       "   342         1          1.0      1.0      0.0                  try:\n",
       "   343         1   17612715.0 17612715.0     79.7                      text = self.lemmatize_text(text)\n",
       "   344                                                           except Exception as excp:\n",
       "   345                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   346                                                               exp = excp.args[0]\n",
       "   347                                           \n",
       "   348         1          2.0      2.0      0.0              if exp is None:\n",
       "   349                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   350         1      40639.0  40639.0      0.2                  text = self.remove_noise(text)\n",
       "   351                                           \n",
       "   352                                                           # Skip documents with no content\n",
       "   353         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   354                                                               # Detect majority language of the document \n",
       "   355         1          0.0      0.0      0.0                      try:\n",
       "   356         1     524749.0 524749.0      2.4                          predict_lang = detect_langs(text)[0]\n",
       "   357                                           \n",
       "   358         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   359                                           \n",
       "   360         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   361         1          4.0      4.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   362         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   363                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   364         1    3408051.0 3408051.0     15.4                                      spell_data = self.spellcheck_text(text)\n",
       "   365         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   366         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   367                                           \n",
       "   368                                                                           # Log tokens count\n",
       "   369         1     202844.0 202844.0      0.9                                  token_log = len(word_tokenize(text))\n",
       "   370         1          2.0      2.0      0.0                                  write_status = True\n",
       "   371                                                                       else:\n",
       "   372                                                                           #not in english\n",
       "   373                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   374                                                                   else:\n",
       "   375                                                                       if self.use_spellchecker:\n",
       "   376                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   377                                                                           spell_data = self.spellcheck_text(text)\n",
       "   378                                                                           spell_errors = spell_data['errors']\n",
       "   379                                                                           text = spell_data['text']\n",
       "   380                                                                           \n",
       "   381                                                                       # Log tokens count\n",
       "   382                                                                       token_log = len(word_tokenize(text))\n",
       "   383                                                                       write_status = True\n",
       "   384                                           \n",
       "   385                                                               except Exception as excp:\n",
       "   386                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   387                                                                   self.logger(skipped_log)\n",
       "   388                                                                   exp = excp.args[0]\n",
       "   389                                                           else:\n",
       "   390                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   391                                                               self.logger(skipped_log)\n",
       "   392                                                               # Log tokens count\n",
       "   393                                                   else:\n",
       "   394         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   395         1         57.0     57.0      0.0              self.logger(skipped_log)\n",
       "   396                                                       # Log tokens count\n",
       "   397         1          1.0      1.0      0.0              token_log = 0\n",
       "   398                                           \n",
       "   399         2          1.0      0.5      0.0          text_log = text\n",
       "   400                                           \n",
       "   401         2          2.0      1.0      0.0          payload = dict(\n",
       "   402         2          2.0      1.0      0.0              lang=lang_log,\n",
       "   403         2          0.0      0.0      0.0              token=token_log,\n",
       "   404         2          2.0      1.0      0.0              text=text_log,\n",
       "   405         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   406         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   407         2          2.0      1.0      0.0              exception=exp,\n",
       "   408         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   409                                                   )\n",
       "   410                                           \n",
       "   411         2         62.0     31.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   412         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 12.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.002425 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1273.0      0.4     52.5          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31                                                       self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1152.0      0.4     47.5          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.014788 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1       5988.0   5988.0     40.5          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1164.0      0.4      7.9          for res in respell_results:\n",
       "    53      3038       1279.0      0.4      8.6              word = res['word']\n",
       "    54      3038       1238.0      0.4      8.4              correct_word = res['correct_word']\n",
       "    55      3038       1228.0      0.4      8.3              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1341.0      0.4      9.1              if correct_word and score > self.spell_threshold:\n",
       "    58       772        384.0      0.5      2.6                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        342.0      0.5      2.3                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        820.0     11.2      5.5                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1002.0      0.4      6.8                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          0.0      0.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 9.49047 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: lemmatize_text_spacy at line 150\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   150                                               def lemmatize_text_spacy(self, text):\n",
       "   151         1         27.0     27.0      0.0          stopwords = set(self.stopwords)\n",
       "   152         1          0.0      0.0      0.0          try:\n",
       "   153         1     216056.0 216056.0      2.3              lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])\n",
       "   154                                                   except OSError:\n",
       "   155                                                       lmtzr_spacy = spacy.load('/R/spacy_data/en_core_web_sm/en_core_web_sm-2.0.0', disable=['parser', 'ner', 'textcat'])\n",
       "   156                                           \n",
       "   157         1    7359889.0 7359889.0     77.6          doc = lmtzr_spacy(text.lower())\n",
       "   158                                                       # ' '.join(re.findall('[a-z0-9]+', text.lower())))\n",
       "   159                                                   \n",
       "   160         1          2.0      2.0      0.0          txt_out = ''\n",
       "   161                                                   \n",
       "   162    195613      93605.0      0.5      1.0          for token in doc:\n",
       "   163    195612     190766.0      1.0      2.0              if token.lemma_ in stopwords:\n",
       "   164     52079      16706.0      0.3      0.2                  continue\n",
       "   165                                                           \n",
       "   166    143533    1613164.0     11.2     17.0              txt_out = txt_out + token.lemma_ + ' '\n",
       "   167                                                       \n",
       "   168         1        236.0    236.0      0.0          txt_out = txt_out.replace('-PRON-', '')\n",
       "   169                                           \n",
       "   170         1         24.0     24.0      0.0          return txt_out.strip()\n",
       "\n",
       "Total time: 0.955777 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: get_misspelled_tokens at line 189\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   189                                               def get_misspelled_tokens(self, text):\n",
       "   190         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   191                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   192                                                   \n",
       "   193         1          2.0      2.0      0.0          errors = set([])\n",
       "   194                                           \n",
       "   195                                           #         if ENCHANT_INSTALLED:\n",
       "   196                                           #             # Input is a text\n",
       "   197                                           #             self.spellchecker.set_text(text)\n",
       "   198                                           \n",
       "   199                                           #             for err in self.spellchecker:\n",
       "   200                                           #                 #print (err.word)\n",
       "   201                                           #                 if err.word not in self.respelled_set:\n",
       "   202                                           #                     errors.add(err.word)\n",
       "   203                                           \n",
       "   204         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   205                                                       # Input is a list of tokens\n",
       "   206         1       3217.0   3217.0      0.3              text_tokens = set(text)\n",
       "   207         1          6.0      6.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   208                                           \n",
       "   209     13333     938021.0     70.4     98.1              for err in self.spellchecker:\n",
       "   210                                                           # print (err.word)\n",
       "   211     13332       7791.0      0.6      0.8                  if err.word not in self.respelled_set:\n",
       "   212     13332       6739.0      0.5      0.7                      errors.add(err.word)\n",
       "   213                                                   else:\n",
       "   214                                                       # Input is a list of tokens\n",
       "   215                                                       text_tokens = set(text)\n",
       "   216                                                       for token in text_tokens:\n",
       "   217                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   218                                                           \n",
       "   219                                                           # If suggestions are available, make sure that the first\n",
       "   220                                                           # suggestion is similar to the token to make sure\n",
       "   221                                                           # that the token being testing is a legit word.\n",
       "   222                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   223                                                               continue\n",
       "   224                                                           else:\n",
       "   225                                                               errors.add(token)\n",
       "   226         1          0.0      0.0      0.0          return errors\n",
       "\n",
       "Total time: 1.64556 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: spellcheck_text at line 229\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   229                                               def spellcheck_text(self, text):\n",
       "   230         1     237926.0 237926.0     14.5          text_tokens=word_tokenize(text)\n",
       "   231         1     969031.0 969031.0     58.9          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   232                                           \n",
       "   233         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   234         1      23024.0  23024.0      1.4              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   235                                                       # print(respelled_set)\n",
       "   236         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   237                                           \n",
       "   238         1         41.0     41.0      0.0          errors_set=set(errors)\n",
       "   239         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   240                                                   \n",
       "   241     79643      24551.0      0.3      1.5          for x in text_tokens:\n",
       "   242     79642      26295.0      0.3      1.6              if (x in errors_set):\n",
       "   243     13001       3786.0      0.3      0.2                  continue\n",
       "   244                                                       \n",
       "   245     66641      23688.0      0.4      1.4              elif x in self.respelled_set:\n",
       "   246       537        216.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   247       206         99.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   248       206         73.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   249                                           \n",
       "   250     66310     282115.0      4.3     17.1              elif (x in self.stopwords):\n",
       "   251         2          1.0      0.5      0.0                  continue\n",
       "   252                                           \n",
       "   253                                                       else:\n",
       "   254     66308      30230.0      0.5      1.8                  x = self.plural_singular_map.get(x, x)\n",
       "   255     66308      23568.0      0.4      1.4                  cleaned_text.append(x)\n",
       "   256                                           \n",
       "   257         1          1.0      1.0      0.0          output={}\n",
       "   258         1        911.0    911.0      0.1          output['text']=\" \".join(cleaned_text)\n",
       "   259         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   260                                              \n",
       "   261         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 12.1843 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: clean_text at line 329\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   329                                               def clean_text(self, text, filen=None):\n",
       "   330         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   331         2          1.0      0.5      0.0          token_log = 0\n",
       "   332         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   333         2          1.0      0.5      0.0          text_log = ''\n",
       "   334         2          1.0      0.5      0.0          spell_errors = []\n",
       "   335         2          1.0      0.5      0.0          exp = None\n",
       "   336         2          2.0      1.0      0.0          write_status = False\n",
       "   337                                                   \n",
       "   338         2          1.0      0.5      0.0          if self.acronym_mapper is not None:\n",
       "   339         2     297716.0 148858.0      2.4              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   340                                           \n",
       "   341         2       5235.0   2617.5      0.0          text = text.lower()\n",
       "   342         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   343                                                   \n",
       "   344         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   345                                                       \n",
       "   346         1          0.0      0.0      0.0              if self.use_lemmatizer:\n",
       "   347                                                           # Apply lemmatizer\n",
       "   348         1          1.0      1.0      0.0                  try:\n",
       "   349         1    9746959.0 9746959.0     80.0                      text = self.lemmatize_text(text)\n",
       "   350                                                           except Exception as excp:\n",
       "   351                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   352                                                               exp = excp.args[0]\n",
       "   353                                           \n",
       "   354         1          2.0      2.0      0.0              if exp is None:\n",
       "   355                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   356         1      39970.0  39970.0      0.3                  text = self.remove_noise(text)\n",
       "   357                                           \n",
       "   358                                                           # Skip documents with no content\n",
       "   359         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   360                                                               # Detect majority language of the document \n",
       "   361         1          1.0      1.0      0.0                      try:\n",
       "   362         1     111085.0 111085.0      0.9                          predict_lang = detect_langs(text)[0]\n",
       "   363                                           \n",
       "   364         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   365                                           \n",
       "   366         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   367         1          4.0      4.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   368         1          0.0      0.0      0.0                                  if self.use_spellchecker:\n",
       "   369                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   370         1    1781219.0 1781219.0     14.6                                      spell_data = self.spellcheck_text(text)\n",
       "   371         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   372         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   373                                           \n",
       "   374                                                                           # Log tokens count\n",
       "   375         1     201961.0 201961.0      1.7                                  token_log = len(word_tokenize(text))\n",
       "   376         1          2.0      2.0      0.0                                  write_status = True\n",
       "   377                                                                       else:\n",
       "   378                                                                           #not in english\n",
       "   379                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   380                                                                   else:\n",
       "   381                                                                       if self.use_spellchecker:\n",
       "   382                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   383                                                                           spell_data = self.spellcheck_text(text)\n",
       "   384                                                                           spell_errors = spell_data['errors']\n",
       "   385                                                                           text = spell_data['text']\n",
       "   386                                                                           \n",
       "   387                                                                       # Log tokens count\n",
       "   388                                                                       token_log = len(word_tokenize(text))\n",
       "   389                                                                       write_status = True\n",
       "   390                                           \n",
       "   391                                                               except Exception as excp:\n",
       "   392                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   393                                                                   self.logger(skipped_log)\n",
       "   394                                                                   exp = excp.args[0]\n",
       "   395                                                           else:\n",
       "   396                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   397                                                               self.logger(skipped_log)\n",
       "   398                                                               # Log tokens count\n",
       "   399                                                   else:\n",
       "   400         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   401         1         61.0     61.0      0.0              self.logger(skipped_log)\n",
       "   402                                                       # Log tokens count\n",
       "   403         1          1.0      1.0      0.0              token_log = 0\n",
       "   404                                           \n",
       "   405         2          1.0      0.5      0.0          text_log = text\n",
       "   406                                           \n",
       "   407         2          2.0      1.0      0.0          payload = dict(\n",
       "   408         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   409         2          2.0      1.0      0.0              token=token_log,\n",
       "   410         2          1.0      0.5      0.0              text=text_log,\n",
       "   411         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   412         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   413         2          1.0      0.5      0.0              exception=exp,\n",
       "   414         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   415                                                   )\n",
       "   416                                           \n",
       "   417         2         67.0     33.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   418         2          1.0      0.5      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.lemmatize_text_spacy -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 23.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 1.46608 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1688.0      0.6      0.1          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30      3038    1459430.0    480.4     99.5              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31      3038       3540.0      1.2      0.2              self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1421.0      0.5      0.1          return self.spell_cache[word]\n",
       "\n",
       "Total time: 1.48423 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1    1475111.0 1475111.0     99.4          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1222.0      0.4      0.1          for res in respell_results:\n",
       "    53      3038       1314.0      0.4      0.1              word = res['word']\n",
       "    54      3038       1253.0      0.4      0.1              correct_word = res['correct_word']\n",
       "    55      3038       1265.0      0.4      0.1              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1358.0      0.4      0.1              if correct_word and score > self.spell_threshold:\n",
       "    58       772        383.0      0.5      0.0                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        372.0      0.5      0.0                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        859.0     11.8      0.1                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1094.0      0.5      0.1                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 0.962209 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: get_misspelled_tokens at line 189\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   189                                               def get_misspelled_tokens(self, text):\n",
       "   190         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   191                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   192                                                   \n",
       "   193         1          2.0      2.0      0.0          errors = set([])\n",
       "   194                                           \n",
       "   195                                           #         if ENCHANT_INSTALLED:\n",
       "   196                                           #             # Input is a text\n",
       "   197                                           #             self.spellchecker.set_text(text)\n",
       "   198                                           \n",
       "   199                                           #             for err in self.spellchecker:\n",
       "   200                                           #                 #print (err.word)\n",
       "   201                                           #                 if err.word not in self.respelled_set:\n",
       "   202                                           #                     errors.add(err.word)\n",
       "   203                                           \n",
       "   204         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   205                                                       # Input is a list of tokens\n",
       "   206         1       3193.0   3193.0      0.3              text_tokens = set(text)\n",
       "   207         1          7.0      7.0      0.0              self.spellchecker.set_tokens(text)\n",
       "   208                                           \n",
       "   209     13333     944486.0     70.8     98.2              for err in self.spellchecker:\n",
       "   210                                                           # print (err.word)\n",
       "   211     13332       7817.0      0.6      0.8                  if err.word not in self.respelled_set:\n",
       "   212     13332       6703.0      0.5      0.7                      errors.add(err.word)\n",
       "   213                                                   else:\n",
       "   214                                                       # Input is a list of tokens\n",
       "   215                                                       text_tokens = set(text)\n",
       "   216                                                       for token in text_tokens:\n",
       "   217                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   218                                                           \n",
       "   219                                                           # If suggestions are available, make sure that the first\n",
       "   220                                                           # suggestion is similar to the token to make sure\n",
       "   221                                                           # that the token being testing is a legit word.\n",
       "   222                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   223                                                               continue\n",
       "   224                                                           else:\n",
       "   225                                                               errors.add(token)\n",
       "   226         1          0.0      0.0      0.0          return errors\n",
       "\n",
       "Total time: 3.12661 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: spellcheck_text at line 229\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   229                                               def spellcheck_text(self, text):\n",
       "   230         1     243098.0 243098.0      7.8          text_tokens=word_tokenize(text)\n",
       "   231         1     975842.0 975842.0     31.2          errors = self.get_misspelled_tokens(text_tokens)  # text if ENCHANT_INSTALLED else text_tokens)\n",
       "   232                                           \n",
       "   233         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   234         1    1492650.0 1492650.0     47.7              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   235                                                       # print(respelled_set)\n",
       "   236         1          5.0      5.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   237                                           \n",
       "   238         1         31.0     31.0      0.0          errors_set=set(errors)\n",
       "   239         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   240                                                   \n",
       "   241     79643      24443.0      0.3      0.8          for x in text_tokens:\n",
       "   242     79642      26354.0      0.3      0.8              if (x in errors_set):\n",
       "   243     13001       3731.0      0.3      0.1                  continue\n",
       "   244                                                       \n",
       "   245     66641      23816.0      0.4      0.8              elif x in self.respelled_set:\n",
       "   246       537        198.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   247       206         84.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   248       206         94.0      0.5      0.0                      cleaned_text.append(x)\n",
       "   249                                           \n",
       "   250     66310     281312.0      4.2      9.0              elif (x in self.stopwords):\n",
       "   251         2          0.0      0.0      0.0                  continue\n",
       "   252                                           \n",
       "   253                                                       else:\n",
       "   254     66308      30390.0      0.5      1.0                  x = self.plural_singular_map.get(x, x)\n",
       "   255     66308      23757.0      0.4      0.8                  cleaned_text.append(x)\n",
       "   256                                           \n",
       "   257         1          0.0      0.0      0.0          output={}\n",
       "   258         1        807.0    807.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   259         1          0.0      0.0      0.0          output['errors']=errors\n",
       "   260                                              \n",
       "   261         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 23.4378 s\n",
       "File: <ipython-input-2-e55e0164b2a3>\n",
       "Function: clean_text at line 329\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   329                                               def clean_text(self, text, filen=None):\n",
       "   330         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   331         2          2.0      1.0      0.0          token_log = 0\n",
       "   332         2          0.0      0.0      0.0          skipped_log = ''\n",
       "   333         2          2.0      1.0      0.0          text_log = ''\n",
       "   334         2          1.0      0.5      0.0          spell_errors = []\n",
       "   335         2          1.0      0.5      0.0          exp = None\n",
       "   336         2          1.0      0.5      0.0          write_status = False\n",
       "   337                                                   \n",
       "   338         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   339         2     306626.0 153313.0      1.3              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   340                                           \n",
       "   341         2       7359.0   3679.5      0.0          text = text.lower()\n",
       "   342         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   343                                                   \n",
       "   344         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   345                                                       \n",
       "   346         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   347                                                           # Apply lemmatizer\n",
       "   348         1          1.0      1.0      0.0                  try:\n",
       "   349         1   19096817.0 19096817.0     81.5                      text = self.lemmatize_text(text)\n",
       "   350                                                           except Exception as excp:\n",
       "   351                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   352                                                               exp = excp.args[0]\n",
       "   353                                           \n",
       "   354         1          2.0      2.0      0.0              if exp is None:\n",
       "   355                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   356         1      40423.0  40423.0      0.2                  text = self.remove_noise(text)\n",
       "   357                                           \n",
       "   358                                                           # Skip documents with no content\n",
       "   359         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   360                                                               # Detect majority language of the document \n",
       "   361         1          0.0      0.0      0.0                      try:\n",
       "   362         1     521424.0 521424.0      2.2                          predict_lang = detect_langs(text)[0]\n",
       "   363                                           \n",
       "   364         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   365                                           \n",
       "   366         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   367         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   368         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   369                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   370         1    3262982.0 3262982.0     13.9                                      spell_data = self.spellcheck_text(text)\n",
       "   371         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   372         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   373                                           \n",
       "   374                                                                           # Log tokens count\n",
       "   375         1     201948.0 201948.0      0.9                                  token_log = len(word_tokenize(text))\n",
       "   376         1          2.0      2.0      0.0                                  write_status = True\n",
       "   377                                                                       else:\n",
       "   378                                                                           #not in english\n",
       "   379                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   380                                                                   else:\n",
       "   381                                                                       if self.use_spellchecker:\n",
       "   382                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   383                                                                           spell_data = self.spellcheck_text(text)\n",
       "   384                                                                           spell_errors = spell_data['errors']\n",
       "   385                                                                           text = spell_data['text']\n",
       "   386                                                                           \n",
       "   387                                                                       # Log tokens count\n",
       "   388                                                                       token_log = len(word_tokenize(text))\n",
       "   389                                                                       write_status = True\n",
       "   390                                           \n",
       "   391                                                               except Exception as excp:\n",
       "   392                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   393                                                                   self.logger(skipped_log)\n",
       "   394                                                                   exp = excp.args[0]\n",
       "   395                                                           else:\n",
       "   396                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   397                                                               self.logger(skipped_log)\n",
       "   398                                                               # Log tokens count\n",
       "   399                                                   else:\n",
       "   400         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   401         1         68.0     68.0      0.0              self.logger(skipped_log)\n",
       "   402                                                       # Log tokens count\n",
       "   403         1          1.0      1.0      0.0              token_log = 0\n",
       "   404                                           \n",
       "   405         2          2.0      1.0      0.0          text_log = text\n",
       "   406                                           \n",
       "   407         2          3.0      1.5      0.0          payload = dict(\n",
       "   408         2          2.0      1.0      0.0              lang=lang_log,\n",
       "   409         2          1.0      0.5      0.0              token=token_log,\n",
       "   410         2          1.0      0.5      0.0              text=text_log,\n",
       "   411         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   412         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   413         2          2.0      1.0      0.0              exception=exp,\n",
       "   414         2          8.0      4.0      0.0              write_status=write_status,\n",
       "   415                                                   )\n",
       "   416                                           \n",
       "   417         2         80.0     40.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   418         2          1.0      0.5      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.42404 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          2.0      2.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        392.0    392.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3405933.0    255.5     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       9471.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8236.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 4.12283 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     239228.0 239228.0      5.8          text_tokens=word_tokenize(text)\n",
       "   220         1    3439527.0 3439527.0     83.4          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   223         1      23041.0  23041.0      0.6              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         28.0     28.0      0.0          errors_set=set(errors)\n",
       "   228         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      24740.0      0.3      0.6          for x in text_tokens:\n",
       "   231     79642      28675.0      0.4      0.7              if (x in errors_set):\n",
       "   232     13001       3786.0      0.3      0.1                  continue\n",
       "   233                                                       \n",
       "   234     66641      24856.0      0.4      0.6              elif x in self.respelled_set:\n",
       "   235       537        253.0      0.5      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206        100.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         50.0      0.2      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     282444.0      4.3      6.9              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      31281.0      0.5      0.8                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      23958.0      0.4      0.6                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          0.0      0.0      0.0          output={}\n",
       "   247         1        857.0    857.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 14.3601 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          2.0      1.0      0.0          token_log = 0\n",
       "   321         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   322         2          2.0      1.0      0.0          text_log = ''\n",
       "   323         2          1.0      0.5      0.0          spell_errors = []\n",
       "   324         2          2.0      1.0      0.0          exp = None\n",
       "   325         2          0.0      0.0      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     298401.0 149200.5      2.1              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       6547.0   3273.5      0.0          text = text.lower()\n",
       "   331         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          4.0      2.0      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          0.0      0.0      0.0                  try:\n",
       "   338         1    9418101.0 9418101.0     65.6                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      51859.0  51859.0      0.4                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          1.0      1.0      0.0                      try:\n",
       "   351         1     117995.0 117995.0      0.8                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   356         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          0.0      0.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1    4261968.0 4261968.0     29.7                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          2.0      2.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     205055.0 205055.0      1.4                                  token_log = len(word_tokenize(text))\n",
       "   365         1          2.0      2.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         61.0     61.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          0.0      0.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          3.0      1.5      0.0          payload = dict(\n",
       "   397         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   398         2          2.0      1.0      0.0              token=token_log,\n",
       "   399         2          2.0      1.0      0.0              text=text_log,\n",
       "   400         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   401         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   402         2          1.0      0.5      0.0              exception=exp,\n",
       "   403         2          6.0      3.0      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         71.0     35.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2          2.0      1.0      0.0          return payload\n",
       "\n",
       "Total time: 0.002297 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1208.0      0.4     52.6          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31                                                       self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1089.0      0.4     47.4          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.014687 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1       5827.0   5827.0     39.7          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          2.0      2.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1194.0      0.4      8.1          for res in respell_results:\n",
       "    53      3038       1323.0      0.4      9.0              word = res['word']\n",
       "    54      3038       1233.0      0.4      8.4              correct_word = res['correct_word']\n",
       "    55      3038       1248.0      0.4      8.5              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1342.0      0.4      9.1              if correct_word and score > self.spell_threshold:\n",
       "    58       772        372.0      0.5      2.5                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        319.0      0.5      2.2                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        816.0     11.2      5.6                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1010.0      0.4      6.9                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          0.0      0.0      0.0          return words, respelled_set"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 26.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.44161 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          1.0      1.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        813.0    813.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3422976.0    256.7     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       9504.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8316.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          0.0      0.0      0.0          return errors\n",
       "\n",
       "Total time: 5.71658 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     244509.0 244509.0      4.3          text_tokens=word_tokenize(text)\n",
       "   220         1    3456991.0 3456991.0     60.5          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   223         1    1593219.0 1593219.0     27.9              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         30.0     30.0      0.0          errors_set=set(errors)\n",
       "   228         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      24839.0      0.3      0.4          for x in text_tokens:\n",
       "   231     79642      28683.0      0.4      0.5              if (x in errors_set):\n",
       "   232     13001       3864.0      0.3      0.1                  continue\n",
       "   233                                                       \n",
       "   234     66641      25041.0      0.4      0.4              elif x in self.respelled_set:\n",
       "   235       537        213.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206         86.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         80.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     283155.0      4.3      5.0              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      31092.0      0.5      0.5                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      23932.0      0.4      0.4                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          1.0      1.0      0.0          output={}\n",
       "   247         1        837.0    837.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 26.1375 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          2.0      1.0      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          2.0      1.0      0.0          token_log = 0\n",
       "   321         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   322         2          1.0      0.5      0.0          text_log = ''\n",
       "   323         2          2.0      1.0      0.0          spell_errors = []\n",
       "   324         2          2.0      1.0      0.0          exp = None\n",
       "   325         2          1.0      0.5      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     309578.0 154789.0      1.2              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       7259.0   3629.5      0.0          text = text.lower()\n",
       "   331         2          5.0      2.5      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          1.0      1.0      0.0                  try:\n",
       "   338         1   19187093.0 19187093.0     73.4                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      40870.0  40870.0      0.2                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          0.0      0.0      0.0                      try:\n",
       "   351         1     531211.0 531211.0      2.0                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   356         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1    5857194.0 5857194.0     22.4                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          2.0      2.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          2.0      2.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     204134.0 204134.0      0.8                                  token_log = len(word_tokenize(text))\n",
       "   365         1          2.0      2.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         57.0     57.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          1.0      1.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          2.0      1.0      0.0          payload = dict(\n",
       "   397         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   398         2          2.0      1.0      0.0              token=token_log,\n",
       "   399         2          1.0      0.5      0.0              text=text_log,\n",
       "   400         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   401         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   402         2          2.0      1.0      0.0              exception=exp,\n",
       "   403         2          7.0      3.5      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         70.0     35.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2          1.0      0.5      0.0          return payload\n",
       "\n",
       "Total time: 1.56658 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1636.0      0.5      0.1          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30      3038    1559873.0    513.5     99.6              payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31      3038       3571.0      1.2      0.2              self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1503.0      0.5      0.1          return self.spell_cache[word]\n",
       "\n",
       "Total time: 1.5849 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1    1575764.0 1575764.0     99.4          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1220.0      0.4      0.1          for res in respell_results:\n",
       "    53      3038       1330.0      0.4      0.1              word = res['word']\n",
       "    54      3038       1294.0      0.4      0.1              correct_word = res['correct_word']\n",
       "    55      3038       1264.0      0.4      0.1              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1385.0      0.5      0.1              if correct_word and score > self.spell_threshold:\n",
       "    58       772        367.0      0.5      0.0                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        343.0      0.5      0.0                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        837.0     11.5      0.1                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1091.0      0.5      0.1                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 14.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.3574 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          2.0      2.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        462.0    462.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3339829.0    250.5     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       8852.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8256.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 4.05873 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     238471.0 238471.0      5.9          text_tokens=word_tokenize(text)\n",
       "   220         1    3372130.0 3372130.0     83.1          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          2.0      2.0      0.0          if errors and self.respeller:\n",
       "   223         1      23622.0  23622.0      0.6              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         29.0     29.0      0.0          errors_set=set(errors)\n",
       "   228         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      25515.0      0.3      0.6          for x in text_tokens:\n",
       "   231     79642      29003.0      0.4      0.7              if (x in errors_set):\n",
       "   232     13001       3812.0      0.3      0.1                  continue\n",
       "   233                                                       \n",
       "   234     66641      24226.0      0.4      0.6              elif x in self.respelled_set:\n",
       "   235       537        222.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206         96.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         82.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     285939.0      4.3      7.0              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      31170.0      0.5      0.8                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      23530.0      0.4      0.6                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          1.0      1.0      0.0          output={}\n",
       "   247         1        873.0    873.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          1.0      1.0      0.0          return output\n",
       "\n",
       "Total time: 14.2874 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          1.0      0.5      0.0          token_log = 0\n",
       "   321         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   322         2          2.0      1.0      0.0          text_log = ''\n",
       "   323         2          1.0      0.5      0.0          spell_errors = []\n",
       "   324         2          2.0      1.0      0.0          exp = None\n",
       "   325         2          2.0      1.0      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          1.0      0.5      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     297715.0 148857.5      2.1              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       5159.0   2579.5      0.0          text = text.lower()\n",
       "   331         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          1.0      1.0      0.0                  try:\n",
       "   338         1    9429043.0 9429043.0     66.0                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      40443.0  40443.0      0.3                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          1.0      1.0      0.0                      try:\n",
       "   351         1     111614.0 111614.0      0.8                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          1.0      1.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          2.0      2.0      0.0                          if self.check_language:\n",
       "   356         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1    4200609.0 4200609.0     29.4                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          2.0      2.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     202575.0 202575.0      1.4                                  token_log = len(word_tokenize(text))\n",
       "   365         1          1.0      1.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         61.0     61.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          1.0      1.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          3.0      1.5      0.0          payload = dict(\n",
       "   397         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   398         2          2.0      1.0      0.0              token=token_log,\n",
       "   399         2          2.0      1.0      0.0              text=text_log,\n",
       "   400         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   401         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   402         2          2.0      1.0      0.0              exception=exp,\n",
       "   403         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         63.0     31.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2         12.0      6.0      0.0          return payload\n",
       "\n",
       "Total time: 0.002206 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038       1152.0      0.4     52.2          if word not in self.spell_cache:\n",
       "    29                                                       # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30                                                       payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31                                                       self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1054.0      0.3     47.8          return self.spell_cache[word]\n",
       "\n",
       "Total time: 0.01468 s\n",
       "File: <ipython-input-2-d39910e15e8b>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          0.0      0.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1       5551.0   5551.0     37.8          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1224.0      0.4      8.3          for res in respell_results:\n",
       "    53      3038       1328.0      0.4      9.0              word = res['word']\n",
       "    54      3038       1316.0      0.4      9.0              correct_word = res['correct_word']\n",
       "    55      3038       1271.0      0.4      8.7              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1366.0      0.4      9.3              if correct_word and score > self.spell_threshold:\n",
       "    58       772        396.0      0.5      2.7                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        368.0      0.5      2.5                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        796.0     10.9      5.4                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1062.0      0.5      7.2                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 47.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 22.9449 s\n",
       "File: <ipython-input-2-9ab183f4984a>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28                                                   # if word not in self.spell_cache:\n",
       "    29                                                   #     # Implement internal caching as well since Memory is still slow due to its utilization of disk.\n",
       "    30      3038   22940043.0   7551.0    100.0          payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    31      3038       3490.0      1.1      0.0          self.spell_cache[word] = payload\n",
       "    32                                           \n",
       "    33      3038       1382.0      0.5      0.0          return self.spell_cache[word]\n",
       "\n",
       "Total time: 22.9621 s\n",
       "File: <ipython-input-2-9ab183f4984a>\n",
       "Function: parallel_infer_correct_word at line 45\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    45                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    46         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    47                                                   \n",
       "    48         1   22953137.0 22953137.0    100.0          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    49                                           \n",
       "    50         1          1.0      1.0      0.0          words = set([])\n",
       "    51                                           \n",
       "    52      3039       1160.0      0.4      0.0          for res in respell_results:\n",
       "    53      3038       1296.0      0.4      0.0              word = res['word']\n",
       "    54      3038       1296.0      0.4      0.0              correct_word = res['correct_word']\n",
       "    55      3038       1256.0      0.4      0.0              score = res['score']\n",
       "    56                                           \n",
       "    57      3038       1313.0      0.4      0.0              if correct_word and score > self.spell_threshold:\n",
       "    58       772        390.0      0.5      0.0                  if correct_word.istitle():\n",
       "    59                                                               # If the respelling results to a `Title` word\n",
       "    60                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    61       699        357.0      0.5      0.0                      words.add(word)\n",
       "    62                                                           else:\n",
       "    63                                                               # Split and filter since some words are compound terms.\n",
       "    64        73        828.0     11.3      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    65                                                       else:\n",
       "    66      2266       1028.0      0.5      0.0                  words.add(word)\n",
       "    67                                           \n",
       "    68         1          1.0      1.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 3.37184 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          0.0      0.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          2.0      2.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        415.0    415.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3354052.0    251.6     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       8968.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8405.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 27.0235 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     244047.0 244047.0      0.9          text_tokens=word_tokenize(text)\n",
       "   220         1    3386845.0 3386845.0     12.5          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   223         1   22970381.0 22970381.0     85.0              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         30.0     30.0      0.0          errors_set=set(errors)\n",
       "   228         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      24990.0      0.3      0.1          for x in text_tokens:\n",
       "   231     79642      29350.0      0.4      0.1              if (x in errors_set):\n",
       "   232     13001       3874.0      0.3      0.0                  continue\n",
       "   233                                                       \n",
       "   234     66641      25404.0      0.4      0.1              elif x in self.respelled_set:\n",
       "   235       537        231.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206         83.0      0.4      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         81.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     281775.0      4.2      1.0              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      31363.0      0.5      0.1                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      24243.0      0.4      0.1                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          1.0      1.0      0.0          output={}\n",
       "   247         1        813.0    813.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 47.3872 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          2.0      1.0      0.0          token_log = 0\n",
       "   321         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   322         2          1.0      0.5      0.0          text_log = ''\n",
       "   323         2          2.0      1.0      0.0          spell_errors = []\n",
       "   324         2          2.0      1.0      0.0          exp = None\n",
       "   325         2          0.0      0.0      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     304633.0 152316.5      0.6              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       7147.0   3573.5      0.0          text = text.lower()\n",
       "   331         2          5.0      2.5      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          0.0      0.0      0.0                  try:\n",
       "   338         1   19135832.0 19135832.0     40.4                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      40994.0  40994.0      0.1                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          1.0      1.0      0.0                      try:\n",
       "   351         1     530057.0 530057.0      1.1                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   356         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1   27165337.0 27165337.0     57.3                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          2.0      2.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     202963.0 202963.0      0.4                                  token_log = len(word_tokenize(text))\n",
       "   365         1          2.0      2.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         59.0     59.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          1.0      1.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          1.0      0.5      0.0          payload = dict(\n",
       "   397         2          2.0      1.0      0.0              lang=lang_log,\n",
       "   398         2          2.0      1.0      0.0              token=token_log,\n",
       "   399         2          0.0      0.0      0.0              text=text_log,\n",
       "   400         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   401         2          1.0      0.5      0.0              spell_errors=spell_errors,\n",
       "   402         2          1.0      0.5      0.0              exception=exp,\n",
       "   403         2          9.0      4.5      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         65.0     32.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 48.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 23.4931 s\n",
       "File: <ipython-input-2-24e404aabc04>\n",
       "Function: infer_correct_word at line 27\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    27                                               def infer_correct_word(self, word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True):\n",
       "    28      3038   23488167.0   7731.5    100.0          payload = cached_infer_correct_word(word, sim_thresh=0.0, print_log=False, min_len=3, use_suggest_score=True)\n",
       "    29      3038       3413.0      1.1      0.0          self.spell_cache[word] = payload\n",
       "    30                                                   \n",
       "    31      3038       1554.0      0.5      0.0          return self.spell_cache[word]\n",
       "\n",
       "Total time: 23.5103 s\n",
       "File: <ipython-input-2-24e404aabc04>\n",
       "Function: parallel_infer_correct_word at line 43\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    43                                               def parallel_infer_correct_word(self, words, num_workers):\n",
       "    44         1          1.0      1.0      0.0          respelled_set = {}\n",
       "    45                                                   \n",
       "    46         1   23501323.0 23501323.0    100.0          respell_results = [self.infer_correct_word(ew) for ew in words]\n",
       "    47                                           \n",
       "    48         1          1.0      1.0      0.0          words = set([])\n",
       "    49                                           \n",
       "    50      3039       1201.0      0.4      0.0          for res in respell_results:\n",
       "    51      3038       1294.0      0.4      0.0              word = res['word']\n",
       "    52      3038       1316.0      0.4      0.0              correct_word = res['correct_word']\n",
       "    53      3038       1259.0      0.4      0.0              score = res['score']\n",
       "    54                                           \n",
       "    55      3038       1339.0      0.4      0.0              if correct_word and score > self.spell_threshold:\n",
       "    56       772        373.0      0.5      0.0                  if correct_word.istitle():\n",
       "    57                                                               # If the respelling results to a `Title` word\n",
       "    58                                                               # it implies that the word is a proper noun, therefore, omit.\n",
       "    59       699        348.0      0.5      0.0                      words.add(word)\n",
       "    60                                                           else:\n",
       "    61                                                               # Split and filter since some words are compound terms.\n",
       "    62        73        809.0     11.1      0.0                      respelled_set[word] = [i for i in correct_word.split() if self.qualified_word(i)]\n",
       "    63                                                       else:\n",
       "    64      2266       1064.0      0.5      0.0                  words.add(word)\n",
       "    65                                           \n",
       "    66         1          0.0      0.0      0.0          return words, respelled_set\n",
       "\n",
       "Total time: 3.45391 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          2.0      2.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          3.0      3.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        435.0    435.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3435753.0    257.7     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       9237.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8484.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 27.6619 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     248897.0 248897.0      0.9          text_tokens=word_tokenize(text)\n",
       "   220         1    3469389.0 3469389.0     12.5          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   223         1   23518599.0 23518599.0     85.0              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1         19.0     19.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         46.0     46.0      0.0          errors_set=set(errors)\n",
       "   228         1          1.0      1.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      25077.0      0.3      0.1          for x in text_tokens:\n",
       "   231     79642      29193.0      0.4      0.1              if (x in errors_set):\n",
       "   232     13001       3991.0      0.3      0.0                  continue\n",
       "   233                                                       \n",
       "   234     66641      25071.0      0.4      0.1              elif x in self.respelled_set:\n",
       "   235       537        240.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206         93.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         69.0      0.3      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     284111.0      4.3      1.0              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      31714.0      0.5      0.1                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      24517.0      0.4      0.1                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          0.0      0.0      0.0          output={}\n",
       "   247         1        823.0    823.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 48.31 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          1.0      0.5      0.0          token_log = 0\n",
       "   321         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   322         2          0.0      0.0      0.0          text_log = ''\n",
       "   323         2          2.0      1.0      0.0          spell_errors = []\n",
       "   324         2          1.0      0.5      0.0          exp = None\n",
       "   325         2          0.0      0.0      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     309306.0 154653.0      0.6              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       7116.0   3558.0      0.0          text = text.lower()\n",
       "   331         2          5.0      2.5      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          2.0      1.0      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          0.0      0.0      0.0                  try:\n",
       "   338         1   19399287.0 19399287.0     40.2                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      40348.0  40348.0      0.1                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          1.0      1.0      0.0                      try:\n",
       "   351         1     536416.0 536416.0      1.1                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   356         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1   27813001.0 27813001.0     57.6                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          1.0      1.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     204325.0 204325.0      0.4                                  token_log = len(word_tokenize(text))\n",
       "   365         1          1.0      1.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         55.0     55.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          0.0      0.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          2.0      1.0      0.0          payload = dict(\n",
       "   397         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   398         2          1.0      0.5      0.0              token=token_log,\n",
       "   399         2          2.0      1.0      0.0              text=text_log,\n",
       "   400         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   401         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   402         2          2.0      1.0      0.0              exception=exp,\n",
       "   403         2          9.0      4.5      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         68.0     34.0      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.parallel_infer_correct_word -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 1min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.40993 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          2.0      2.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          1.0      1.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        448.0    448.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3391536.0    254.4     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       9446.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8498.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          1.0      1.0      0.0          return errors\n",
       "\n",
       "Total time: 74.824 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     245857.0 245857.0      0.3          text_tokens=word_tokenize(text)\n",
       "   220         1    3425985.0 3425985.0      4.6          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   223         1   70730889.0 70730889.0     94.5              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         28.0     28.0      0.0          errors_set=set(errors)\n",
       "   228         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      24939.0      0.3      0.0          for x in text_tokens:\n",
       "   231     79642      28637.0      0.4      0.0              if (x in errors_set):\n",
       "   232     13001       3856.0      0.3      0.0                  continue\n",
       "   233                                                       \n",
       "   234     66641      24691.0      0.4      0.0              elif x in self.respelled_set:\n",
       "   235       537        221.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206         95.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         69.0      0.3      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     283280.0      4.3      0.4              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      30440.0      0.5      0.0                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      24186.0      0.4      0.0                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          1.0      1.0      0.0          output={}\n",
       "   247         1        821.0    821.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          0.0      0.0      0.0          return output\n",
       "\n",
       "Total time: 94.943 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          4.0      2.0      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          2.0      1.0      0.0          token_log = 0\n",
       "   321         2          1.0      0.5      0.0          skipped_log = ''\n",
       "   322         2          1.0      0.5      0.0          text_log = ''\n",
       "   323         2          2.0      1.0      0.0          spell_errors = []\n",
       "   324         2          1.0      0.5      0.0          exp = None\n",
       "   325         2          2.0      1.0      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     305437.0 152718.5      0.3              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       7143.0   3571.5      0.0          text = text.lower()\n",
       "   331         2          4.0      2.0      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          0.0      0.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          1.0      1.0      0.0                  try:\n",
       "   338         1   18910783.0 18910783.0     19.9                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      40616.0  40616.0      0.0                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          1.0      1.0      0.0                      try:\n",
       "   351         1     508321.0 508321.0      0.5                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   356         1          4.0      4.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1   74965784.0 74965784.0     79.0                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          2.0      2.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          1.0      1.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     204762.0 204762.0      0.2                                  token_log = len(word_tokenize(text))\n",
       "   365         1          1.0      1.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         70.0     70.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          1.0      1.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          2.0      1.0      0.0          payload = dict(\n",
       "   397         2          1.0      0.5      0.0              lang=lang_log,\n",
       "   398         2          2.0      1.0      0.0              token=token_log,\n",
       "   399         2          2.0      1.0      0.0              text=text_log,\n",
       "   400         2          1.0      0.5      0.0              skipped=skipped_log,\n",
       "   401         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   402         2          2.0      1.0      0.0              exception=exp,\n",
       "   403         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         63.0     31.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2          1.0      0.5      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens -f Respeller.infer_correct_word cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doclen 18 < 50 = None\n",
      "time: 14.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 3.39493 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: get_misspelled_tokens at line 188\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   188                                               def get_misspelled_tokens(self, text):\n",
       "   189         1          1.0      1.0      0.0          if self.spellchecker is None:\n",
       "   190                                                       raise ValueError('Spellchecker is not enabled')\n",
       "   191                                                   \n",
       "   192         1          1.0      1.0      0.0          errors = set([])\n",
       "   193                                           \n",
       "   194         1          0.0      0.0      0.0          if ENCHANT_INSTALLED:\n",
       "   195                                                       # Input is a text\n",
       "   196         1        493.0    493.0      0.0              self.spellchecker.set_text(text)\n",
       "   197                                           \n",
       "   198     13333    3376535.0    253.2     99.5              for err in self.spellchecker:\n",
       "   199                                                           #print (err.word)\n",
       "   200     13332       9450.0      0.7      0.3                  if err.word not in self.respelled_set:\n",
       "   201     13332       8449.0      0.6      0.2                      errors.add(err.word)\n",
       "   202                                                   else:\n",
       "   203                                                       # Input is a list of tokens\n",
       "   204                                                       text_tokens = set(text)\n",
       "   205                                                       for token in text_tokens:\n",
       "   206                                                           suggestions = self.spellchecker.suggest(token)\n",
       "   207                                                           \n",
       "   208                                                           # If suggestions are available, make sure that the first\n",
       "   209                                                           # suggestion is similar to the token to make sure\n",
       "   210                                                           # that the token being testing is a legit word.\n",
       "   211                                                           if suggestions and (suggestions[0][0] == token and suggestions[0][1] > 0.9):\n",
       "   212                                                               continue\n",
       "   213                                                           else:\n",
       "   214                                                               errors.add(token)\n",
       "   215         1          0.0      0.0      0.0          return errors\n",
       "\n",
       "Total time: 4.28543 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: spellcheck_text at line 218\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   218                                               def spellcheck_text(self, text):\n",
       "   219         1     242826.0 242826.0      5.7          text_tokens=word_tokenize(text)\n",
       "   220         1    3410938.0 3410938.0     79.6          errors = self.get_misspelled_tokens(text if ENCHANT_INSTALLED else text_tokens)\n",
       "   221                                           \n",
       "   222         1          1.0      1.0      0.0          if errors and self.respeller:\n",
       "   223         1     207673.0 207673.0      4.8              errors, respelled_set = self.respeller.parallel_infer_correct_word(errors, self.num_workers * 2)  # max((self.num_workers // 2), 1))\n",
       "   224                                                       # print(respelled_set)\n",
       "   225         1          4.0      4.0      0.0              self.respelled_set.update(respelled_set)\n",
       "   226                                           \n",
       "   227         1         28.0     28.0      0.0          errors_set=set(errors)\n",
       "   228         1          0.0      0.0      0.0          cleaned_text = []\n",
       "   229                                                   \n",
       "   230     79643      25421.0      0.3      0.6          for x in text_tokens:\n",
       "   231     79642      29325.0      0.4      0.7              if (x in errors_set):\n",
       "   232     13001       3805.0      0.3      0.1                  continue\n",
       "   233                                                       \n",
       "   234     66641      24740.0      0.4      0.6              elif x in self.respelled_set:\n",
       "   235       537        217.0      0.4      0.0                  for x in self.respelled_set[x]:\n",
       "   236       206        101.0      0.5      0.0                      x = self.plural_singular_map.get(x, x)\n",
       "   237       206         78.0      0.4      0.0                      cleaned_text.append(x)\n",
       "   238                                           \n",
       "   239     66310     284508.0      4.3      6.6              elif (x in self.stopwords):\n",
       "   240         2          1.0      0.5      0.0                  continue\n",
       "   241                                           \n",
       "   242                                                       else:\n",
       "   243     66308      30702.0      0.5      0.7                  x = self.plural_singular_map.get(x, x)\n",
       "   244     66308      23955.0      0.4      0.6                  cleaned_text.append(x)\n",
       "   245                                           \n",
       "   246         1          0.0      0.0      0.0          output={}\n",
       "   247         1       1106.0   1106.0      0.0          output['text']=\" \".join(cleaned_text)\n",
       "   248         1          1.0      1.0      0.0          output['errors']=errors\n",
       "   249                                              \n",
       "   250         1          1.0      1.0      0.0          return output\n",
       "\n",
       "Total time: 14.5349 s\n",
       "File: <ipython-input-2-d28491b177a4>\n",
       "Function: clean_text at line 318\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   318                                               def clean_text(self, text, filen=None):\n",
       "   319         2          3.0      1.5      0.0          lang_log = ('ERROR', 0)\n",
       "   320         2          1.0      0.5      0.0          token_log = 0\n",
       "   321         2          2.0      1.0      0.0          skipped_log = ''\n",
       "   322         2          1.0      0.5      0.0          text_log = ''\n",
       "   323         2          2.0      1.0      0.0          spell_errors = []\n",
       "   324         2          1.0      0.5      0.0          exp = None\n",
       "   325         2          0.0      0.0      0.0          write_status = False\n",
       "   326                                                   \n",
       "   327         2          2.0      1.0      0.0          if self.acronym_mapper is not None:\n",
       "   328         2     294020.0 147010.0      2.0              text = self.acronym_mapper.expand_doc_acronyms(text)\n",
       "   329                                           \n",
       "   330         2       5059.0   2529.5      0.0          text = text.lower()\n",
       "   331         2          3.0      1.5      0.0          len_text = len(text)\n",
       "   332                                                   \n",
       "   333         2          3.0      1.5      0.0          if len_text > self.ignore_length:\n",
       "   334                                                       \n",
       "   335         1          1.0      1.0      0.0              if self.use_lemmatizer:\n",
       "   336                                                           # Apply lemmatizer\n",
       "   337         1          1.0      1.0      0.0                  try:\n",
       "   338         1    9453176.0 9453176.0     65.0                      text = self.lemmatize_text(text)\n",
       "   339                                                           except Exception as excp:\n",
       "   340                                                               self.logger(f'Failed lemmatization for {filen}')\n",
       "   341                                                               exp = excp.args[0]\n",
       "   342                                           \n",
       "   343         1          2.0      2.0      0.0              if exp is None:\n",
       "   344                                                           # Remove noise words e.g. punctuation, numbers, non-utf characters etc\n",
       "   345         1      40857.0  40857.0      0.3                  text = self.remove_noise(text)\n",
       "   346                                           \n",
       "   347                                                           # Skip documents with no content\n",
       "   348         1          3.0      3.0      0.0                  if len(text) > 0:      \n",
       "   349                                                               # Detect majority language of the document \n",
       "   350         1          0.0      0.0      0.0                      try:\n",
       "   351         1     108497.0 108497.0      0.7                          predict_lang = detect_langs(text)[0]\n",
       "   352                                           \n",
       "   353         1          2.0      2.0      0.0                          lang_log = (predict_lang.lang, predict_lang.prob)\n",
       "   354                                           \n",
       "   355         1          1.0      1.0      0.0                          if self.check_language:\n",
       "   356         1          3.0      3.0      0.0                              if (any([predict_lang.lang == lg for lg in self.supported_lang])) and (predict_lang.prob >= self.min_en_lang_prob):   # Only process documents in English                            \n",
       "   357         1          1.0      1.0      0.0                                  if self.use_spellchecker:\n",
       "   358                                                                               # Run spell check and keep only the words found in dictionary\n",
       "   359         1    4427002.0 4427002.0     30.5                                      spell_data = self.spellcheck_text(text)\n",
       "   360         1          2.0      2.0      0.0                                      spell_errors = spell_data['errors']\n",
       "   361         1          2.0      2.0      0.0                                      text = spell_data['text']\n",
       "   362                                           \n",
       "   363                                                                           # Log tokens count\n",
       "   364         1     206154.0 206154.0      1.4                                  token_log = len(word_tokenize(text))\n",
       "   365         1          2.0      2.0      0.0                                  write_status = True\n",
       "   366                                                                       else:\n",
       "   367                                                                           #not in english\n",
       "   368                                                                           skipped_log = f'Not in english | {predict_lang}'\n",
       "   369                                                                   else:\n",
       "   370                                                                       if self.use_spellchecker:\n",
       "   371                                                                           # Run spell check and keep only the words found in dictionary\n",
       "   372                                                                           spell_data = self.spellcheck_text(text)\n",
       "   373                                                                           spell_errors = spell_data['errors']\n",
       "   374                                                                           text = spell_data['text']\n",
       "   375                                                                           \n",
       "   376                                                                       # Log tokens count\n",
       "   377                                                                       token_log = len(word_tokenize(text))\n",
       "   378                                                                       write_status = True\n",
       "   379                                           \n",
       "   380                                                               except Exception as excp:\n",
       "   381                                                                   skipped_log = f\"Error detecting language for = {filen}. {excp.args[0]}\"\n",
       "   382                                                                   self.logger(skipped_log)\n",
       "   383                                                                   exp = excp.args[0]\n",
       "   384                                                           else:\n",
       "   385                                                               skipped_log = f\"Empty doc post lemmatizer = {filen}\"\n",
       "   386                                                               self.logger(skipped_log)\n",
       "   387                                                               # Log tokens count\n",
       "   388                                                   else:\n",
       "   389         1          3.0      3.0      0.0              skipped_log = f\"Doclen {len_text} < {self.ignore_length} = {filen}\"\n",
       "   390         1         47.0     47.0      0.0              self.logger(skipped_log)\n",
       "   391                                                       # Log tokens count\n",
       "   392         1          1.0      1.0      0.0              token_log = 0\n",
       "   393                                           \n",
       "   394         2          2.0      1.0      0.0          text_log = text\n",
       "   395                                           \n",
       "   396         2          2.0      1.0      0.0          payload = dict(\n",
       "   397         2          2.0      1.0      0.0              lang=lang_log,\n",
       "   398         2          2.0      1.0      0.0              token=token_log,\n",
       "   399         2          1.0      0.5      0.0              text=text_log,\n",
       "   400         2          2.0      1.0      0.0              skipped=skipped_log,\n",
       "   401         2          2.0      1.0      0.0              spell_errors=spell_errors,\n",
       "   402         2          2.0      1.0      0.0              exception=exp,\n",
       "   403         2          5.0      2.5      0.0              write_status=write_status,\n",
       "   404                                                   )\n",
       "   405                                           \n",
       "   406         2         59.0     29.5      0.0          payload = {k: v if not isinstance(v, set) else list(v) for k, v in payload.items()}\n",
       "   407         2          2.0      1.0      0.0          return payload"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f Cleaner.clean_text -f Cleaner.spellcheck_text -f Cleaner.get_misspelled_tokens cleaner_test('25693850.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.6 ms\n"
     ]
    }
   ],
   "source": [
    "path = '/home/avsolatorio/WBG/NLP/WB/CORPUS/RAW/eap_files'\n",
    "doc_name = '25693850.txt'\n",
    "fname = os.path.join(path, doc_name)\n",
    "\n",
    "with open(fname, 'rb') as fl:\n",
    "    text = fl.read()\n",
    "    text = text.decode('utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 363 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Cleaner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d9e94cf90a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cleaner=Cleaner(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0muse_spellchecker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_respeller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_lemmatizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_spacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreplacements_plurals_to_singular_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../whitelists/whitelist_replacements_plurals_to_singular.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0macronyms_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../whitelists/whitelist_acronyms.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Cleaner' is not defined"
     ]
    }
   ],
   "source": [
    "cleaner=Cleaner(\n",
    "    use_spellchecker=True, use_respeller=True, use_lemmatizer=True, use_spacy=True,\n",
    "    replacements_plurals_to_singular_file='../whitelists/whitelist_replacements_plurals_to_singular.csv',\n",
    "    acronyms_file='../whitelists/whitelist_acronyms.csv',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "doc = lmtzr_spacy(cleaner.space_normalize_text(text).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1628156"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.73 ms\n"
     ]
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 218 ms\n"
     ]
    }
   ],
   "source": [
    "lmtzr_spacy = spacy.load('en', disable=['parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 964 µs\n"
     ]
    }
   ],
   "source": [
    "def doc_gen(text, n=5):\n",
    "    for _ in range(n):\n",
    "        yield cleaner.space_normalize_text(text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.02 ms\n"
     ]
    }
   ],
   "source": [
    "docs = doc_gen(text, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "def token_filter(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) <= 4)\n",
    "\n",
    "for doc in lmtzr_spacy.pipe(\n",
    "    [cleaner.space_normalize_text(text).lower(),\n",
    "     cleaner.space_normalize_text(text).lower(),\n",
    "     cleaner.space_normalize_text(text).lower()], n_threads=8, batch_size=10000):\n",
    "    \n",
    "    tokens = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.8 s\n"
     ]
    }
   ],
   "source": [
    "for doc in lmtzr_spacy.pipe(docs, n_threads=8, batch_size=10000):\n",
    "\n",
    "    tokens = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl (29.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 29.8MB 3.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jsonschema<3.1.0,>=2.6.0 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from spacy) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from spacy) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from spacy) (0.9.6)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e2/4db8df8f6cddc98e7d7c537245ef2f4e41a1ed17bf0c3177ab3cc6beac7f/numpy-1.16.3-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 5.7MB/s \n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.0.5 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 32.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from spacy) (2.0.1)\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 34.5MB/s \n",
      "\u001b[?25hCollecting thinc<7.1.0,>=7.0.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.1MB 44.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /home/avsolatorio/anaconda3/lib/python3.6/site-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.23.4)\n",
      "\u001b[31mtensorflow-gpu 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.16.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: wasabi, numpy, srsly, blis, thinc, spacy\n",
      "  Found existing installation: numpy 1.14.5\n",
      "    Uninstalling numpy-1.14.5:\n",
      "      Successfully uninstalled numpy-1.14.5\n",
      "  Found existing installation: thinc 6.12.0\n",
      "    Uninstalling thinc-6.12.0:\n",
      "      Successfully uninstalled thinc-6.12.0\n",
      "  Found existing installation: spacy 2.0.16\n",
      "    Uninstalling spacy-2.0.16:\n",
      "      Successfully uninstalled spacy-2.0.16\n",
      "Successfully installed blis-0.2.4 numpy-1.16.3 spacy-2.1.4 srsly-0.0.5 thinc-7.0.4 wasabi-0.2.2\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/home/avsolatorio/anaconda3/bin/pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}